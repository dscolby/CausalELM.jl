var documenterSearchIndex = {"docs":
[{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"These release notes adhere to the keep a changelog format. Below is a list of changes since CausalELM was first released.","category":"page"},{"location":"release_notes/#Version-[v0.8.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.6.1)-2024-10-28","page":"Release Notes","title":"Version v0.8.0 - 2024-10-28","text":"","category":"section"},{"location":"release_notes/#Fixed","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Removed unnecessary include and using statements","category":"page"},{"location":"release_notes/#Version-[v0.7.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.7.0)-2024-06-22","page":"Release Notes","title":"Version v0.7.0 - 2024-06-22","text":"","category":"section"},{"location":"release_notes/#Added","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Implemented bagged ensemble of extreme learning machines to use with estimators #67\nImplemented multithreading for testing the sensitivity of estimators to the counterfactual consistency assumption","category":"page"},{"location":"release_notes/#Changed","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Compute the number of neurons to use with log heuristic instead of cross validation #62\nCalculate probabilities as the average label predicted by the ensemble instead of clipping #71\nMade calculation of p-values and standard errors optional and not executed by default in summarize methods #65\nRemoved redundant W argument for double machine learning, R-learning, and doubly robust estimation #68\nUse swish as the default activation function #72\nImplemented noise as a function of each observation instead of the variance of the outcome when testing the sensitivity of the counterfactual consistency assumption #74\np-values and standard errors for randomization inference are generated in parallel","category":"page"},{"location":"release_notes/#Fixed-2","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Applying the weight trick for R-learning #70","category":"page"},{"location":"release_notes/#Version-[v0.6.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.6.0)-2024-06-15","page":"Release Notes","title":"Version v0.6.0 - 2024-06-15","text":"","category":"section"},{"location":"release_notes/#Added-2","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Implemented doubly robust learner for CATE estimation #31\nProvided better explanations of supported treatment and outcome variable types in the docs #41\nAdded support for specifying confounders, W, separate from covariates of interest, X, for double machine ","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"learning and doubly robust estimation 39","category":"page"},{"location":"release_notes/#Changed-2","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Removed the estimatecausaleffect! call in the model constructor docstrings #35\nStandardized and improved docstrings and added doctests #44\nCounterfactual consistency now simulates outcomes that violate the counterfactual consistency assumption rather than ","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"binning of treatments and works with discrete or continuous treatments #33","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Refactored estimator and metalearner structs, constructors, and estimatecausaleffect! methods #45","category":"page"},{"location":"release_notes/#Fixed-3","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Clipped probabilities between 0 and 1 for estimators that use predictions of binary variables #36\nFixed sample splitting and cross fitting procedure for doubly robust estimation #42\nAddressed numerical instability when finding the ridge penalty by replacing the previous ridge formula with ","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"generalized cross validation #43","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Uses the correct variable in the ommited predictor test for interrupted time series.\nUses correct range for p-values in interrupted time series validation tests.\nCorrectly subsets the data for ATT estimation in G-computation #52","category":"page"},{"location":"release_notes/#Version-[v0.5.1](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.5.1)-2024-01-15","page":"Release Notes","title":"Version v0.5.1 - 2024-01-15","text":"","category":"section"},{"location":"release_notes/#Added-3","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"More descriptive docstrings #21","category":"page"},{"location":"release_notes/#Fixed-4","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Permutation of continuous treatments draws from a continuous, instead of discrete uniform distribution during randomization inference","category":"page"},{"location":"release_notes/#Version-[v0.5.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.5.0)-2024-01-13","page":"Release Notes","title":"Version v0.5.0 - 2024-01-13","text":"","category":"section"},{"location":"release_notes/#Added-4","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Constructors for estimators taht accept dataframes from DataFrames.jl #25","category":"page"},{"location":"release_notes/#Changed-3","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Estimators can handle any array whose values are <:Real #23\nEstimator constructors are now called with model(X, T, Y) instead of model(X, Y, T)\nRemoved excess type constraints for many methods #23\nVectorized a few for loops\nIncreased test coverage","category":"page"},{"location":"release_notes/#Version-[v0.4.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.4.0)-2024-01-06","page":"Release Notes","title":"Version v0.4.0 - 2024-01-06","text":"","category":"section"},{"location":"release_notes/#Added-5","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"R-learning\nSoftmax function for arrays","category":"page"},{"location":"release_notes/#Changed-4","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Moved all types and methods under the main module\nDecreased size of function definitions #22\nSLearner has a G-computation field that does the heavy lifting for S-learning\nRemoved excess fields from estimator structs","category":"page"},{"location":"release_notes/#Fixed-5","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Changed the incorrect name of DoublyRobustEstimation struct to DoubleMachineLearning\nCaclulation of risk ratios and E-values\nCalculation of validation metrics for multiclass classification\nCalculation of output weights for L2 regularized extreme learning machines","category":"page"},{"location":"release_notes/#Version-[v0.3.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.3.0)-2023-11-25","page":"Release Notes","title":"Version v0.3.0 - 2023-11-25","text":"","category":"section"},{"location":"release_notes/#Added-6","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Splitting of temporal data for cross validation 18\nMethods to validate/test senstivity to violations of identifying assumptions #16","category":"page"},{"location":"release_notes/#Changed-5","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Converted all functions and methods to snake case #17\nRandomization inference for interrupted time series randomizes all the indices #15","category":"page"},{"location":"release_notes/#Fixed-6","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Issue related to recoding variables to calculate validation metrics for cross validation","category":"page"},{"location":"release_notes/#Version-[v0.2.1](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.2.1)-2023-06-07","page":"Release Notes","title":"Version v0.2.1 - 2023-06-07","text":"","category":"section"},{"location":"release_notes/#Added-7","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Cross fitting to the doubly robust estimator","category":"page"},{"location":"release_notes/#Version-[v0.2.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.2.0)-2023-04-16","page":"Release Notes","title":"Version v0.2.0 - 2023-04-16","text":"","category":"section"},{"location":"release_notes/#Added-8","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Calculation of p-values and standard errors via randomization inference","category":"page"},{"location":"release_notes/#Changed-6","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Divided package into modules","category":"page"},{"location":"release_notes/#Version-[v0.1.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.1.0)-2023-02-14","page":"Release Notes","title":"Version v0.1.0 - 2023-02-14","text":"","category":"section"},{"location":"release_notes/#Added-9","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Event study, g-computation, and doubly robust estimators\nS-learning, T-learning, and X-learning\nModel summarization methods","category":"page"},{"location":"api/#CausalELM","page":"API","title":"CausalELM","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.CausalELM","category":"page"},{"location":"api/#CausalELM.CausalELM","page":"API","title":"CausalELM.CausalELM","text":"Macros, functions, and structs for applying Ensembles of extreme learning machines to causal  inference tasks where the counterfactual is unavailable or biased and must be predicted.  Supports causal inference via interrupted time series designs, parametric G-computation,  double machine learning, and S-learning, T-learning, X-learning, R-learning, and doubly  robust estimation.\n\nFor more details on Extreme Learning Machines see:     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\n\n\n\n\n","category":"module"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"InterruptedTimeSeries\nGComputation\nDoubleMachineLearning\nSLearner\nTLearner\nXLearner\nRLearner\nDoublyRobustLearner\nCausalELM.CausalEstimator\nCausalELM.Metalearner\nCausalELM.ExtremeLearner\nCausalELM.ELMEnsemble\nCausalELM.Nonbinary\nCausalELM.Binary\nCausalELM.Count\nCausalELM.Continuous","category":"page"},{"location":"api/#CausalELM.InterruptedTimeSeries","page":"API","title":"CausalELM.InterruptedTimeSeries","text":"InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; kwargs...)\n\nInitialize an interrupted time series estimator. \n\nArguments\n\nX₀::Any: array or DataFrame of covariates from the pre-treatment period.\nY₁::Any: array or DataFrame of outcomes from the pre-treatment period.\nX₁::Any: array or DataFrame of covariates from the post-treatment period.\nY₁::Any: array or DataFrame of outcomes from the post-treatment period.\n\nKeywords\n\nactivation::Function=swish: activation function to use.\nsample_size::Integer=size(X₀, 1): number of bootstrapped samples for the extreme    learner.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X₀, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor a simple linear regression-based tutorial on interrupted time series analysis see:     Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. \"Interrupted time series      regression for the evaluation of public health interventions: a tutorial.\" International      journal of epidemiology 46, no. 1 (2017): 348-355.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> m2 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; regularized=false)\njulia> x₀_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100))\njulia> y₀_df = DataFrame(y=rand(100))\njulia> x₁_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100)) \njulia> y₁_df = DataFrame(y=rand(100))\njulia> m3 = InterruptedTimeSeries(x₀_df, y₀_df, x₁_df, y₁_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.GComputation","page":"API","title":"CausalELM.GComputation","text":"GComputation(X, T, Y; kwargs...)\n\nInitialize a G-Computation estimator.\n\nArguments\n\nX::Any: array or DataFrame of covariates.\nT::Any: vector or DataFrame of treatment statuses.\nY::Any: array or DataFrame of outcomes.\n\nKeywords\n\nquantity_of_interest::String: ATE for average treatment effect or ATT for average    treatment effect on the treated.\nactivation::Function=swish: activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for the extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor a good overview of G-Computation see:     Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé      Rousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann      Foucher. \"G-computation, propensity score-based methods, and targeted maximum likelihood      estimator for causal inference with different covariates sets: a comparative simulation      study.\" Scientific reports 10, no. 1 (2020): 9219.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, T, Y)\njulia> m2 = GComputation(X, T, Y; task=\"regression\")\njulia> m3 = GComputation(X, T, Y; task=\"regression\", quantity_of_interest=\"ATE)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100)) \njulia> m5 = GComputation(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.DoubleMachineLearning","page":"API","title":"CausalELM.DoubleMachineLearning","text":"DoubleMachineLearning(X, T, Y; kwargs...)\n\nInitialize a double machine learning estimator with cross fitting.\n\nArguments\n\nX::Any: array or DataFrame of covariates of interest.\nT::Any: vector or DataFrame of treatment statuses.\nY::Any: array or DataFrame of outcomes.\n\nKeywords\n\nactivation::Function=swish: activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for teh extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75, * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\nfolds::Integer: number of folds to use for cross fitting.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor more information see:     Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,      Whitney Newey, and James Robins. \"Double/debiased machine learning for treatment and      structural parameters.\" (2016): C1-C68.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m2 = DoubleMachineLearning(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.SLearner","page":"API","title":"CausalELM.SLearner","text":"SLearner(X, T, Y; kwargs...)\n\nInitialize a S-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nactivation::Function=swish: the activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for eth extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor an overview of S-Learners and other metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of      the national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = SLearner(X, T, Y)\njulia> m2 = SLearner(X, T, Y; task=\"regression\")\njulia> m3 = SLearner(X, T, Y; task=\"regression\", regularized=true)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m4 = SLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.TLearner","page":"API","title":"CausalELM.TLearner","text":"TLearner(X, T, Y; kwargs...)\n\nInitialize a T-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nactivation::Function=swish: the activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for eth extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor an overview of T-Learners and other metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of      the national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = TLearner(X, T, Y)\njulia> m2 = TLearner(X, T, Y; regularized=false)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m3 = TLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.XLearner","page":"API","title":"CausalELM.XLearner","text":"XLearner(X, T, Y; kwargs...)\n\nInitialize an X-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nactivation::Function=swish: the activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for eth extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor an overview of X-Learners and other metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> m2 = XLearner(X, T, Y; regularized=false)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m3 = XLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.RLearner","page":"API","title":"CausalELM.RLearner","text":"RLearner(X, T, Y; kwargs...)\n\nInitialize an R-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates of interest.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nactivation::Function=swish: the activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for eth extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor an explanation of R-Learner estimation see:     Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment      effects.\" Biometrika 108, no. 2 (2021): 299-319.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = RLearner(X, T, Y)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m2 = RLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.DoublyRobustLearner","page":"API","title":"CausalELM.DoublyRobustLearner","text":"DoublyRobustLearner(X, T, Y; kwargs...)\n\nInitialize a doubly robust CATE estimator.\n\nArguments\n\nX::Any: an array or DataFrame of covariates of interest.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nactivation::Function=swish: the activation function to use.\nsample_size::Integer=size(X, 1): number of bootstrapped samples for eth extreme    learners.\nnum_machines::Integer=50: number of extreme learning machines for the ensemble.\nnum_feats::Integer=Int(round(0.75 * size(X, 2))): number of features to bootstrap for    each learner in the ensemble.\nnum_neurons::Integer: number of neurons to use in the extreme learning machines.\n\nNotes\n\nTo reduce the computational complexity you can reduce samplesize, nummachines, or  num_neurons.\n\nReferences\n\nFor an explanation of doubly robust cate estimation see:     Kennedy, Edward H. \"Towards optimal doubly robust estimation of heterogeneous causal      effects.\" Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoublyRobustLearner(X, T, Y)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m2 = DoublyRobustLearner(x_df, t_df, y_df)\n\njulia> w = rand(100, 6)\njulia> m3 = DoublyRobustLearner(X, T, Y, W=w)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.CausalEstimator","page":"API","title":"CausalELM.CausalEstimator","text":"Abstract type for GComputation and DoubleMachineLearning\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Metalearner","page":"API","title":"CausalELM.Metalearner","text":"Abstract type for metalearners\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.ExtremeLearner","page":"API","title":"CausalELM.ExtremeLearner","text":"ExtremeLearner(X, Y, hidden_neurons, activation)\n\nConstruct an ExtremeLearner for fitting and prediction.\n\nNotes\n\nWhile it is possible to use an ExtremeLearner for regression, it is recommended to use  RegularizedExtremeLearner, which imposes an L2 penalty, to reduce multicollinearity.\n\nReferences\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.ELMEnsemble","page":"API","title":"CausalELM.ELMEnsemble","text":"ELMEnsemble(X, Y, sample_size, num_machines, num_neurons)\n\nInitialize a bagging ensemble of extreme learning machines. \n\nArguments\n\nX::Array{Float64}: array of features for predicting labels.\nY::Array{Float64}: array of labels to predict.\nsample_size::Integer: how many data points to use for each extreme learning machine.\nnum_machines::Integer: how many extreme learning machines to use.\nnum_feats::Integer: how many features to consider for eac exreme learning machine.\nnum_neurons::Integer: how many neurons to use for each extreme learning machine.\nactivation::Function: activation function to use for the extreme learning machines.\n\nNotes\n\nELMEnsemble uses the same bagging approach as random forests when the labels are continuous  but uses the average predicted probability, rather than voting, for classification.\n\nExamples\n\njulia> X, Y =  rand(100, 5), rand(100)\njulia> m1 = ELMEnsemble(X, Y, 10, 50, 5, 5, CausalELM.relu)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Nonbinary","page":"API","title":"CausalELM.Nonbinary","text":"Abstract type used to dispatch risk_ratio on nonbinary treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Binary","page":"API","title":"CausalELM.Binary","text":"Type used to dispatch risk_ratio on binary treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Count","page":"API","title":"CausalELM.Count","text":"Type used to dispatch risk_ratio on count treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Continuous","page":"API","title":"CausalELM.Continuous","text":"Type used to dispatch risk_ratio on continuous treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#Activation-Functions","page":"API","title":"Activation Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"binary_step\nσ\nCausalELM.tanh\nrelu\nleaky_relu\nswish\nsoftmax\nsoftplus\ngelu\ngaussian\nhard_tanh\nelish\nfourier","category":"page"},{"location":"api/#CausalELM.binary_step","page":"API","title":"CausalELM.binary_step","text":"binary_step(x)\n\nApply the binary step activation function.\n\nExamples\n\njulia> binary_step(1)\n1\n\njulia> binary_step([-1000, 100, 1, 0, -0.001, -3])\n6-element Vector{Int64}:\n 0\n 1\n 1\n 1\n 0\n 0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.σ","page":"API","title":"CausalELM.σ","text":"σ(x)\n\nApply the sigmoid activation function.\n\nExamples\n\njulia> σ(1)\n0.7310585786300049\n\njulia> σ([1.0, 0.0])\n2-element Vector{Float64}:\n 0.7310585786300049\n 0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.tanh","page":"API","title":"CausalELM.tanh","text":"tanh(x)\n\nApply the hyperbolic tangent activation function.\n\nExamples\n\njulia> CausalELM.tanh([1.0, 0.0])\n2-element Vector{Float64}:\n 0.7615941559557649\n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.relu","page":"API","title":"CausalELM.relu","text":"relu(x)\n\nApply the ReLU activation function.\n\nExamples\n\njulia> relu(1)\n1\n\njulia> relu([1.0, 0.0, -1.0])\n3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.leaky_relu","page":"API","title":"CausalELM.leaky_relu","text":"leaky_relu(x)\n\nApply the leaky ReLU activation function to a number.\n\nExamples\n\njulia> leaky_relu(1)\n1\n\njulia> leaky_relu([-1.0, 0.0, 1.0])\n3-element Vector{Float64}:\n -0.01\n  0.0\n  1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.swish","page":"API","title":"CausalELM.swish","text":"swish(x)\n\nApply the swish activation function to a number.\n\nExamples\n\njulia> swish(1)\n0.7310585786300049\n\njulia> swish([1.0, -1.0])\n2-element Vector{Float64}:\n  0.7310585786300049\n -0.2689414213699951\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.softmax","page":"API","title":"CausalELM.softmax","text":"softmax(x)\n\nApply the softmax activation function to a number.\n\nExamples\n\njulia> softmax(1)\n1.0\n\njulia> softmax([1.0, 2.0, 3.0])\n3-element Vector{Float64}:\n 0.09003057317038045\n 0.24472847105479764\n 0.6652409557748219\n\njulia> softmax([1.0 2.0 3.0; 4.0 5.0 6.0])\n2×3 Matrix{Float64}:\n 0.0900306  0.244728  0.665241\n 0.0900306  0.244728  0.665241\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.softplus","page":"API","title":"CausalELM.softplus","text":"softplus(x)\n\nApply the softplus activation function to a number.\n\nExamples\n\njulia> softplus(1)\n1.3132616875182228\n\njulia> softplus([1.0, -1.0])\n2-element Vector{Float64}:\n 1.3132616875182228\n 0.3132616875182228\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gelu","page":"API","title":"CausalELM.gelu","text":"gelu(x)\n\nApply the GeLU activation function to a number.\n\nExamples\n\njulia> gelu(1)\n0.8411919906082768\n\njulia> gelu([-1.0, 0.0])\n2-element Vector{Float64}:\n -0.15880800939172324\n  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gaussian","page":"API","title":"CausalELM.gaussian","text":"gaussian(x)\n\nApply the gaussian activation function to a real number.\n\nExamples\n\njulia> gaussian(1)\n0.36787944117144233\n\njulia> gaussian([1.0, -1.0])\n2-element Vector{Float64}:\n 0.3678794411714423\n 0.3678794411714423\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.hard_tanh","page":"API","title":"CausalELM.hard_tanh","text":"hard_tanh(x)\n\nApply the hard_tanh activation function to a number.\n\nExamples\n\njulia> hard_tanh(-2)\n-1\n\njulia> hard_tanh([-2.0, 0.0, 2.0])\n3-element Vector{Real}:\n -1\n  0.0\n  1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.elish","page":"API","title":"CausalELM.elish","text":"elish(x)\n\nApply the ELiSH activation function to a number.\n\nExamples\n\njulia> elish(1)\n0.7310585786300049\n\njulia> elish([-1.0, 1.0])\n2-element Vector{Float64}:\n -0.17000340156854793\n  0.7310585786300049\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.fourier","page":"API","title":"CausalELM.fourier","text":"fourrier(x)\n\nApply the Fourier activation function to a real number.\n\nExamples\n\njulia> fourier(1)\n0.8414709848078965\n\njulia> fourier([-1.0, 1.0])\n2-element Vector{Float64}:\n -0.8414709848078965\n  0.8414709848078965\n\n\n\n\n\n","category":"function"},{"location":"api/#Average-Causal-Effect-Estimators","page":"API","title":"Average Causal Effect Estimators","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.g_formula!\nCausalELM.predict_residuals\nCausalELM.moving_average","category":"page"},{"location":"api/#CausalELM.g_formula!","page":"API","title":"CausalELM.g_formula!","text":"g_formula!(g)\n\nCompute the G-formula for G-computation and S-learning.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = GComputation(X, T, Y)\njulia> g_formula!(m1)\n\njulia> m2 = SLearner(X, T, Y)\njulia> g_formula!(m2)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict_residuals","page":"API","title":"CausalELM.predict_residuals","text":"predict_residuals(D, x_train, x_test, y_train, y_test, t_train, t_test)\n\nPredict treatment and outcome residuals for double machine learning or R-learning.\n\nNotes\n\nThis method should not be called directly.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> x_train, x_test = X[1:80, :], X[81:end, :]\njulia> y_train, y_test = Y[1:80], Y[81:end]\njulia> t_train, t_test = T[1:80], T[81:100]\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> predict_residuals(m1, x_train, x_test, y_train, y_test, t_train, t_test)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.moving_average","page":"API","title":"CausalELM.moving_average","text":"moving_average(x)\n\nCalculates a cumulative moving average.\n\nExamples\n\njulia> moving_average([1, 2, 3])\n\n\n\n\n\n","category":"function"},{"location":"api/#Metalearners","page":"API","title":"Metalearners","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.doubly_robust_formula!\nCausalELM.stage1!\nCausalELM.stage2!","category":"page"},{"location":"api/#CausalELM.doubly_robust_formula!","page":"API","title":"CausalELM.doubly_robust_formula!","text":"doubly_robust_formula!(DRE, X, T, Y)\n\nEstimate the CATE for a single cross fitting iteration via doubly robust estimation.\n\nNotes\n\nThis method should not be called directly.\n\nArguments\n\nDRE::DoublyRobustLearner: the DoubleMachineLearning struct to estimate the effect for.\nX: a vector of three covariate folds.\nT: a vector of three treatment folds.\nY: a vector of three outcome folds.\n\nExamples\n\njulia> X, T, Y, W =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100), rand(6, 100)\njulia> m1 = DoublyRobustLearner(X, T, Y)\n\njulia> X, T, W, Y = make_folds(m1)\njulia> Z = m1.W == m1.X ? X : [reduce(hcat, (z)) for z in zip(X, W)]\njulia> g_formula!(m1, X, T, Y, Z)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.stage1!","page":"API","title":"CausalELM.stage1!","text":"stage1!(x)\n\nEstimate the first stage models for an X-learner.\n\nNotes\n\nThis method should not be called by the user.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> stage1!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.stage2!","page":"API","title":"CausalELM.stage2!","text":"stage2!(x)\n\nEstimate the second stage models for an X-learner.\n\nNotes\n\nThis method should not be called by the user.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> stage1!(m1)\njulia> stage2!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Common-Methods","page":"API","title":"Common Methods","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"estimate_causal_effect!","category":"page"},{"location":"api/#CausalELM.estimate_causal_effect!","page":"API","title":"CausalELM.estimate_causal_effect!","text":"estimate_causal_effect!(its)\n\nEstimate the effect of an event relative to a predicted counterfactual.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(g)\n\nEstimate a causal effect of interest using G-Computation.\n\nNotes\n\nIf treatents are administered at multiple time periods, the effect will be estimated as the  average difference between the outcome of being treated in all periods and being treated in  no periods. For example, given that ividuals 1, 2, ..., i ∈ I recieved either a treatment  or a placebo in p different periods, the model would estimate the average treatment effect  as E[Yᵢ|T₁=1, T₂=1, ... Tₚ=1, Xₚ] - E[Yᵢ|T₁=0, T₂=0, ... Tₚ=0, Xₚ].\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = GComputation(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(DML)\n\nEstimate a causal effect of interest using double machine learning.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\njulia> W = rand(100, 6)\njulia> m2 = DoubleMachineLearning(X, T, Y, W=W)\njulia> estimate_causal_effect!(m2)\n\n\n\n\n\nestimate_causal_effect!(s)\n\nEstimate the CATE using an S-learner.\n\nReferences\n\nFor an overview of S-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m4 = SLearner(X, T, Y)\njulia> estimate_causal_effect!(m4)\n\n\n\n\n\nestimate_causal_effect!(t)\n\nEstimate the CATE using an T-learner.\n\nReferences\n\nFor an overview of T-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m5 = TLearner(X, T, Y)\njulia> estimate_causal_effect!(m5)\n\n\n\n\n\nestimate_causal_effect!(x)\n\nEstimate the CATE using an X-learner.\n\nReferences\n\nFor an overview of X-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(R)\n\nEstimate the CATE using an R-learner.\n\nReferences\n\nFor an overview of R-learning see:     Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment      effects.\" Biometrika 108, no. 2 (2021): 299-319.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = RLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(DRE)\n\nEstimate the CATE using a doubly robust learner.\n\nReferences\n\nFor details on how this method estimates the CATE see:     Kennedy, Edward H. \"Towards optimal doubly robust estimation of heterogeneous causal      effects.\" Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoublyRobustLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Inference","page":"API","title":"Inference","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"summarize\nCausalELM.generate_null_distribution\nCausalELM.quantities_of_interest","category":"page"},{"location":"api/#CausalELM.summarize","page":"API","title":"CausalELM.summarize","text":"summarize(mod, kwargs...)\n\nGet a summary from a CausalEstimator or Metalearner.\n\nArguments\n\nmod::Union{CausalEstimator, Metalearner}: a model to summarize.\n\nKeywords\n\nn::Int=100: the number of iterations to generate the numll distribution for    randomization inference.\ninference::Bool=false: wheteher calculate p-values and standard errors.\n\nNotes\n\np-values and standard errors are estimated using approximate randomization inference. If set  to true, this procedure takes a VERY long time due to repeated matrix inversions.\n\nReferences\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> X, T, Y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = GComputation(X, T, Y)\njulia> estimate_causal_effect!(m1)\njulia> summarize(m1)\n\njulia> m2 = RLearner(X, T, Y)\njulia> estimate_causal_effect(m2)\njulia> summarize(m2)\n\njulia> m3 = SLearner(X, T, Y)\njulia> estimate_causal_effect!(m3)\njulia> summarise(m3)  # British spelling works too!\n\n\n\n\n\nsummarize(its, kwargs...)\n\nGet a summary from an interrupted time series estimator.\n\nArguments\n\nits::InterruptedTimeSeries: interrupted time series estimator\n\nKeywords\n\nn::Int=100: number of iterations to generate the numll distribution for randomization    inference.\nmean_effect::Bool=true: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.\ninference::Bool=false: wheteher calculate p-values and standard errors.\n\nNotes\n\np-values and standard errors are estimated using approximate randomization inference. If set  to true, this procedure takes a VERY long time due to repeated matrix inversions.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m4 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m4)\njulia> summarize(m4)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.generate_null_distribution","page":"API","title":"CausalELM.generate_null_distribution","text":"generate_null_distribution(mod, n)\n\nGenerate a null distribution for the treatment effect of G-computation, double machine  learning, or metalearning.\n\nArguments\n\nmod::Any: model to summarize.\nn::Int=100: number of iterations to generate the null distribution for randomization    inference.\n\nNotes\n\nThis method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When mod is a metalearner the null statistic is the difference is the ATE.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nExamples\n\njulia> x, t, y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(1:100, 100, 1)\njulia> g_computer = GComputation(x, t, y)\njulia> estimate_causal_effect!(g_computer)\njulia> generate_null_distribution(g_computer, 500)\n\n\n\n\n\ngenerate_null_distribution(its, n, mean_effect)\n\nArguments\n\nits::InterruptedTimeSeries: interrupted time series estimator\nn::Int=100: number of iterations to generate the numll distribution for randomization    inference.\nmean_effect::Bool=true: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> generate_null_distribution(its, 10)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.quantities_of_interest","page":"API","title":"CausalELM.quantities_of_interest","text":"quantities_of_interest(mod, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x, t, y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(1:100, 100, 1)\njulia> g_computer = GComputation(x, t, y)\njulia> estimate_causal_effect!(g_computer)\njulia> quantities_of_interest(g_computer, 1000)\n\n\n\n\n\nquantities_of_interest(mod, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> quantities_of_interest(its, 10)\n\n\n\n\n\n","category":"function"},{"location":"api/#Model-Validation","page":"API","title":"Model Validation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"validate\nCausalELM.covariate_independence\nCausalELM.omitted_predictor\nCausalELM.sup_wald\nCausalELM.p_val\nCausalELM.counterfactual_consistency\nCausalELM.simulate_counterfactual_violations\nCausalELM.exchangeability\nCausalELM.e_value\nCausalELM.binarize\nCausalELM.risk_ratio\nCausalELM.positivity","category":"page"},{"location":"api/#CausalELM.validate","page":"API","title":"CausalELM.validate","text":"validate(its; kwargs...)\n\nTest the validity of an estimated interrupted time series analysis.\n\nArguments\n\nits::InterruptedTimeSeries: an interrupted time seiries estimator.\n\nKeywords\n\nn::Int: number of times to simulate a confounder.\nlow::Float64=0.15: minimum proportion of data points to include before or after the    tested break in the Wald supremum test.\nhigh::Float64=0.85: maximum proportion of data points to include before or after the    tested break in the Wald supremum test.\n\nNotes\n\nThis method coducts a Chow Test, a Wald supremeum test, and tests the model's sensitivity to  confounders. The Chow Test tests for structural breaks in the covariates between the time  before and after the event. p-values represent the proportion of times the magnitude of the  break in a covariate would have been greater due to chance. Lower p-values suggest a higher  probability the event effected the covariates and they cannot provide unbiased  counterfactual predictions. The Wald supremum test finds the structural break with the  highest Wald statistic. If this is not the same as the hypothesized break, it could indicate  an anticipation effect, a confounding event, or that the intervention or policy took place  in multiple phases. p-values represent the proportion of times we would see a larger Wald  statistic if the data points were randomly allocated to pre and post-event periods for the  predicted structural break. Ideally, the hypothesized break will be the same as the  predicted break and it will also have a low p-value. The omitted predictors test adds  normal random variables with uniform noise as predictors. If the included covariates are  good predictors of the counterfactual outcome, adding irrelevant predictors should not have  a large effect on the predicted counterfactual outcomes or the estimated effect.\n\nThis method does not implement the second test in Baicker and Svoronos because the estimator  in this package models the relationship between covariates and the outcome and uses an  extreme learning machine instead of linear regression, so variance in the outcome across  different bins is not much of an issue.\n\nReferences\n\nFor more details on the assumptions and validity of interrupted time series designs, see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ = rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\njulia> validate(m1)\n\n\n\n\n\nvalidate(m; kwargs)\n\nArguments\n\nm::Union{CausalEstimator, Metalearner}: model to validate/test the assumptions of.\n\nKeywords\n\ndevs=::Any: iterable of deviations from which to generate noise to simulate violations    of the counterfactual consistency assumption.\nnum_iterations=10::Int: number of times to simulate a violation of the counterfactual    consistency assumption.\nmin::Float64=1.0e-6: minimum probability of treatment for the positivity assumption.\nhigh::Float64=1-min: maximum probability of treatment for the positivity assumption.\n\nNotes\n\nThis method tests the counterfactual consistency, exchangeability, and positivity  assumptions required for causal inference. It should be noted that consistency and  exchangeability are not directly testable, so instead, these tests do not provide definitive  evidence of a violation of these assumptions. To probe the counterfactual consistency  assumption, we simulate counterfactual outcomes that are different from the observed  outcomes, estimate models with the simulated counterfactual outcomes, and take the averages. If the outcome is continuous, the noise for the simulated counterfactuals is drawn from  N(0, dev) for each element in devs, otherwise the default is 0.25, 0.5, 0.75, and 1.0  standard deviations from the mean outcome. For discrete variables, each outcome is replaced  with a different value in the range of outcomes with probability ϵ for each ϵ in devs,  otherwise the default is 0.025, 0.05, 0.075, 0.1. If the average estimate for a given level  of violation differs greatly from the effect estimated on the actual data, then the model is  very sensitive to violations of the counterfactual consistency assumption for that level of  violation. Next, this methods tests the model's sensitivity to a violation of the  exchangeability assumption by calculating the E-value, which is the minimum strength of  association, on the risk ratio scale, that an unobserved confounder would need to have with  the treatment and outcome variable to fully explain away the estimated effect. Thus, higher  E-values imply the model is more robust to a violation of the exchangeability assumption.  Finally, this method tests the positivity assumption by estimating propensity scores. Rows  in the matrix are levels of covariates that have a zero probability of treatment. If the  matrix is empty, none of the observations have an estimated zero probability of treatment,  which implies the positivity assumption is satisfied.\n\nReferences\n\nFor a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. \n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]), vec(rand(1:100, 100, 1)) \njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> validate(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.covariate_independence","page":"API","title":"CausalELM.covariate_independence","text":"covariate_independence(its; kwargs..)\n\nTest for independence between covariates and the event or intervention.\n\nArguments\n\nits::InterruptedTImeSeries: an interrupted time series estimator.\n\nKeywords\n\nn::Int: number of permutations for assigning observations to the pre and        post-treatment periods.\n\nThis is a Chow Test for covariates with p-values estimated via randomization inference,  which does not assume a distribution for the outcome variable. The p-values are the  proportion of times randomly assigning observations to the pre or post-intervention period  would have a larger estimated effect on the the slope of the covariates. The lower the  p-values, the more likely it is that the event/intervention effected the covariates and  they cannot provide an unbiased prediction of the counterfactual outcomes.\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n       randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> covariate_independence(its)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.omitted_predictor","page":"API","title":"CausalELM.omitted_predictor","text":"omitted_predictor(its; kwargs...)\n\nSee how an omitted predictor/variable could change the results of an interrupted time series  analysis.\n\nArguments\n\nits::InterruptedTImeSeries: interrupted time seiries estimator.\n\nKeywords\n\nn::Int: number of times to simulate a confounder.\n\nNotes\n\nThis method reestimates interrupted time series models with uniform random variables. If the  included covariates are good predictors of the counterfactual outcome, adding a random  variable as a covariate should not have a large effect on the predicted counterfactual  outcomes and therefore the estimated average effect.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> omitted_predictor(its)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.sup_wald","page":"API","title":"CausalELM.sup_wald","text":"sup_wald(its; kwargs)\n\nCheck if the predicted structural break is the hypothesized structural break.\n\nArguments\n\nits::InterruptedTimeSeries: interrupted time seiries estimator.\n\nKeywords\n\nn::Int: number of times to simulate a confounder.\nlow::Float64=0.15: minimum proportion of data points to include before or after the        tested break in the Wald supremum test.\nhigh::Float64=0.85: maximum proportion of data points to include before or after the        tested break in the Wald supremum test.\n\nNotes\n\nThis method conducts Wald tests and identifies the structural break with the highest Wald  statistic. If this break is not the same as the hypothesized break, it could indicate an  anticipation effect, confounding by some other event or intervention, or that the  intervention or policy took place in multiple phases. p-values are estimated using  approximate randomization inference and represent the proportion of times we would see a  larger Wald statistic if the data points were randomly allocated to pre and post-event  periods for the predicted structural break.\n\nReferences\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n       randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> sup_wald(its)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.p_val","page":"API","title":"CausalELM.p_val","text":"p_val(x, y, β; kwargs...)\n\nEstimate the p-value for the hypothesis that an event had a statistically significant effect  on the slope of a covariate using randomization inference.\n\nArguments\n\nx::Array{<:Real}: covariates.\ny::Array{<:Real}: outcome.\nβ::Array{<:Real}=0.15: fitted weights.\n\nKeywords\n\ntwo_sided::Bool=false: whether to conduct a one-sided hypothesis test.\n\nExamples\n\njulia> x, y, β = reduce(hcat, (float(rand(0:1, 10)), ones(10))), rand(10), 0.5\njulia> p_val(x, y, β)\njulia> p_val(x, y, β; n=100, two_sided=true)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.counterfactual_consistency","page":"API","title":"CausalELM.counterfactual_consistency","text":"counterfactual_consistency(m; kwargs...)\n\nArguments\n\nm::Union{CausalEstimator, Metalearner}: model to validate/test the assumptions of.\n\nKeywords\n\nnum_devs=(0.25, 0.5, 0.75, 1.0)::Tuple: number of standard deviations from which to    generate noise from a normal distribution to simulate violations of the counterfactual    consistency assumption.\nnum_iterations=10::Int: number of times to simulate a violation of the counterfactual    consistency assumption.\n\nNotes\n\nExamine the counterfactual consistency assumption. First, this function simulates  counterfactual outcomes that are offset from the outcomes in the dataset by random scalars drawn from a N(0, numstddev). Then, the procedure is repeated numiterations times and  averaged. If the model is a metalearner, then the estimated individual treatment effects  are averaged and the mean CATE is averaged over all the iterations, otherwise the estimated  treatment effect is averaged over the iterations. The previous steps are repeated for each  element in numdevs.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> counterfactual_consistency(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.simulate_counterfactual_violations","page":"API","title":"CausalELM.simulate_counterfactual_violations","text":"simulate_counterfactual_violations(y, dev)\n\nArguments\n\ny::Vector{<:Real}: vector of real-valued outcomes.\ndev::Float64: deviation of the observed outcomes from the true counterfactual outcomes.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]), vec(rand(1:100, 100, 1)) \njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> simulate_counterfactual_violations(g_computer)\n-0.7748591231872396\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.exchangeability","page":"API","title":"CausalELM.exchangeability","text":"exchangeability(model)\n\nTest the sensitivity of a G-computation or doubly robust estimator or metalearner to a  violation of the exchangeability assumption.\n\nReferences\n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> e_value(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.e_value","page":"API","title":"CausalELM.e_value","text":"e_value(model)\n\nTest the sensitivity of an estimator to a violation of the exchangeability assumption.\n\nReferences\n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> e_value(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.binarize","page":"API","title":"CausalELM.binarize","text":"binarize(x, cutoff)\n\nConvert a vector of counts or a continuous vector to a binary vector.\n\nArguments\n\nx::Any: interable of numbers to binarize.\nx::Any: threshold after which numbers are converted to 1 and befrore which are converted    to 0.\n\nExamples\n\njulia> CausalELM.binarize([1, 2, 3], 2)\n3-element Vector{Int64}:\n 0\n 0\n 1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.risk_ratio","page":"API","title":"CausalELM.risk_ratio","text":"risk_ratio(model)\n\nCalculate the risk ratio for an estimated model.\n\nNotes\n\nIf the treatment variable is not binary and the outcome variable is not continuous then the  treatment variable will be binarized.\n\nReferences\n\nFor more information on how other quantities of interest are converted to risk ratios see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> risk_ratio(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.positivity","page":"API","title":"CausalELM.positivity","text":"positivity(model, [,min], [,max])\n\nFind likely violations of the positivity assumption.\n\nNotes\n\nThis method uses an extreme learning machine or regularized extreme learning machine to  estimate probabilities of treatment. The returned matrix, which may be empty, are the  covariates that have a (near) zero probability of treatment or near zero probability of  being assigned to the control group, whith their entry in the last column being their  estimated treatment probability. In other words, they likely violate the positivity  assumption.\n\nArguments\n\nmodel::Union{CausalEstimator, Metalearner}: a model to validate/test the assumptions of.\nmin::Float64=1.0e-6: minimum probability of treatment for the positivity assumption.\nhigh::Float64=1-min: the maximum probability of treatment for the positivity assumption.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> positivity(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#Validation-Metrics","page":"API","title":"Validation Metrics","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"mse\nmae\naccuracy\nCausalELM.precision\nrecall\nF1\nCausalELM.confusion_matrix","category":"page"},{"location":"api/#CausalELM.mse","page":"API","title":"CausalELM.mse","text":"mse(y, ŷ)\n\nCalculate the mean squared error\n\nSee also mae.\n\nExamples\n\njulia> mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n4.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.mae","page":"API","title":"CausalELM.mae","text":"mae(y, ŷ)\n\nCalculate the mean absolute error\n\nSee also mse.\n\nExamples\n\njulia> mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n2.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.accuracy","page":"API","title":"CausalELM.accuracy","text":"accuracy(y, ŷ)\n\nCalculate the accuracy for a classification task\n\nExamples\n\njulia> accuracy([1, 1, 1, 1], [0, 1, 1, 0])\n0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.precision","page":"API","title":"CausalELM.precision","text":"precision(y, ŷ)\n\nCalculate the precision for a classification task\n\nSee also recall.\n\nExamples\n\njulia> CausalELM.precision([0, 1, 0, 0], [0, 1, 1, 0])\n1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.recall","page":"API","title":"CausalELM.recall","text":"recall(y, ŷ)\n\nCalculate the recall for a classification task\n\nSee also CausalELM.precision.\n\nExamples\n\njulia> recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.F1","page":"API","title":"CausalELM.F1","text":"F1(y, ŷ)\n\nCalculate the F1 score for a classification task\n\nExamples\n\njulia> F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.confusion_matrix","page":"API","title":"CausalELM.confusion_matrix","text":"confusion_matrix(y, ŷ)\n\nGenerate a confusion matrix\n\nExamples\n\njulia> CausalELM.confusion_matrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\n\n\n\n\n","category":"function"},{"location":"api/#Extreme-Learning-Machines","page":"API","title":"Extreme Learning Machines","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.fit!\nCausalELM.predict\nCausalELM.predict_counterfactual!\nCausalELM.placebo_test\nCausalELM.set_weights_biases","category":"page"},{"location":"api/#CausalELM.fit!","page":"API","title":"CausalELM.fit!","text":"fit!(model)\n\nFit an ExtremeLearner to the data.\n\nReferences\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\n\n\n\n\n\nfit!(model)\n\nFit an ensemble of ExtremeLearners to the data. \n\nArguments\n\nmodel::ELMEnsemble: ensemble of ExtremeLearners to fit.\n\nNotes\n\nThis uses the same bagging approach as random forests when the labels are continuous but  uses the average predicted probability, rather than voting, for classification.\n\nExamples\n\njulia> X, Y =  rand(100, 5), rand(100)\njulia> m1 = ELMEnsemble(X, Y, 10, 50, 5, CausalELM.relu)\njulia> fit!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict","page":"API","title":"CausalELM.predict","text":"predict(model, X)\n\nUse an ExtremeLearningMachine or ELMEnsemble to make predictions.\n\nNotes\n\nIf using an ensemble to make predictions, this method returns a maxtirs where each row is a prediction and each column is a model.\n\nReferences\n\nFor more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\njulia> fit!(m1, sigmoid)\njulia> predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n\njulia> m2 = ELMEnsemble(X, Y, 10, 50, 5, CausalELM.relu)\njulia> fit!(m2)\njulia> predict(m2)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict_counterfactual!","page":"API","title":"CausalELM.predict_counterfactual!","text":"predict_counterfactual!(model, X)\n\nUse an ExtremeLearningMachine to predict the counterfactual.\n\nNotes\n\nThis should be run with the observed covariates. To use synthtic data for what-if scenarios  use predict.\n\nSee also predict.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\njulia> f1 = fit(m1, sigmoid)\njulia> predict_counterfactual!(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.placebo_test","page":"API","title":"CausalELM.placebo_test","text":"placebo_test(model)\n\nConduct a placebo test.\n\nNotes\n\nThis method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\njulia> f1 = fit(m1, sigmoid)\njulia> predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\njulia> placebo_test(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.set_weights_biases","page":"API","title":"CausalELM.set_weights_biases","text":"set_weights_biases(model)\n\nCalculate the weights and biases for an extreme learning machine.\n\nNotes\n\nInitialization is done using uniform Xavier initialization.\n\nReferences\n\nFor details see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\njulia> set_weights_biases(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Utility-Functions","page":"API","title":"Utility Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.var_type\nCausalELM.mean\nCausalELM.var\nCausalELM.one_hot_encode\nCausalELM.clip_if_binary\nCausalELM.@model_config\nCausalELM.@standard_input_data\nCausalELM.generate_folds","category":"page"},{"location":"api/#CausalELM.var_type","page":"API","title":"CausalELM.var_type","text":"var_type(x)\n\nDetermine the type of variable held by a vector.\n\nExamples\n\njulia> CausalELM.var_type([1, 2, 3, 2, 3, 1, 1, 3, 2])\nCausalELM.Count()\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.mean","page":"API","title":"CausalELM.mean","text":"mean(x)\n\nCalculate the mean of a vector.\n\nExamples\n\njulia> CausalELM.mean([1, 2, 3, 4])\n2.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.var","page":"API","title":"CausalELM.var","text":"var(x)\n\nCalculate the (sample) mean of a vector.\n\nExamples\n\njulia> CausalELM.var([1, 2, 3, 4])\n1.6666666666666667\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.one_hot_encode","page":"API","title":"CausalELM.one_hot_encode","text":"one_hot_encode(x)\n\nOne hot encode a categorical vector for multiclass classification.\n\nExamples\n\njulia> CausalELM.one_hot_encode([1, 2, 3, 4, 5])\n5×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.clip_if_binary","page":"API","title":"CausalELM.clip_if_binary","text":"clip_if_binary(x, var)\n\nConstrain binary values between 1e-7 and 1 - 1e-7, otherwise return the original values.\n\nArguments\n\nx::Array: array to clip if it is binary.\nvar: type of x based on calling var_type.\n\nSee also var_type.\n\nExamples\n\njulia> CausalELM.clip_if_binary([1.2, -0.02], CausalELM.Binary())\n2-element Vector{Float64}:\n 1.0\n 0.0\n\njulia> CausalELM.clip_if_binary([1.2, -0.02], CausalELM.Count())\n2-element Vector{Float64}:\n  1.2\n -0.02\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.@model_config","page":"API","title":"CausalELM.@model_config","text":"model_config(effect_type)\n\nGenerate fields common to all CausalEstimator, Metalearner, and InterruptedTimeSeries  structs.\n\nArguments\n\neffect_type::String: \"averageeffect\" or \"individualeffect\" to define fields for either    models that estimate average effects or the CATE.\n\nExamples\n\njulia> struct TestStruct CausalELM.@model_config average_effect end\njulia> TestStruct(\"ATE\", false, \"classification\", true, relu, F1, 2, 10, 5, 100, 5, 5, 0.25)\nTestStruct(\"ATE\", false, \"classification\", true, relu, F1, 2, 10, 5, 100, 5, 5, 0.25)\n\n\n\n\n\n","category":"macro"},{"location":"api/#CausalELM.@standard_input_data","page":"API","title":"CausalELM.@standard_input_data","text":"standard_input_data()\n\nGenerate fields common to all CausalEstimators except DoubleMachineLearning and all  Metalearners except RLearner and DoublyRobustLearner.\n\nExamples\n\njulia> struct TestStruct CausalELM.@standard_input_data end\njulia> TestStruct([5.2], [0.8], [0.96])\nTestStruct([5.2], [0.8], [0.96])\n\n\n\n\n\n","category":"macro"},{"location":"api/#CausalELM.generate_folds","page":"API","title":"CausalELM.generate_folds","text":"generate_folds(X, T, Y, folds)\n\nCreate folds for cross validation.\n\nExamples\n\njulia> xfolds, tfolds, yfolds = CausalELM.generate_folds(zeros(4, 2), zeros(4), ones(4), 2)\n([[0.0 0.0], [0.0 0.0; 0.0 0.0; 0.0 0.0]], [[0.0], [0.0, 0.0, 0.0]], [[1.0], [1.0, 1.0, 1.0]])\n\n\n\n\n\n","category":"function"},{"location":"guide/estimatorselection/#Deciding-Which-Estimator-to-Use","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"","category":"section"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"Which model you should use depends on what you are trying to model and the type of data you  have. The table below can serve as a useful reference when deciding which model to use for a  given dataset and causal question.","category":"page"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"Model Struct Causal Estimands Supported Treatment Types Supported Outcome Types\nInterrupted Time Series Analysis InterruptedTimeSeries ATE, Cumulative Treatment Effect Binary Continuous, Count[1], Time to Event\nG-computation GComputation ATE, ATT, ITT Binary Binary,Continuous, Time to Event, Count[1]\nDouble Machine Learning DoubleMachineLearning ATE Binary, Count[1], Continuous Binary, Count[1], Continuous, Time to Event\nS-learning SLearner CATE Binary Binary, Continuous, Time to Event, Count[1]\nT-learning TLearner CATE Binary Binary, Continuous, Count[1], Time to Event\nX-learning XLearner CATE Binary Binary, Continuous, Count[1], Time to Event\nR-learning RLearner CATE Binary, Count[1], Continuous Binary, Count[1], Continuous, Time to Event\nDoubly Robust Estimation DoublyRobustLearner CATE Binary Binary, Continuous, Count[1], Time to Event","category":"page"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"[1]: Similar to other packages, predictions of count variables is treated as a continuous regression task.","category":"page"},{"location":"guide/gcomputation/#G-Computation","page":"G-computation","title":"G-Computation","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"In some cases, we may want to know the causal effect of a treatment that varies and is  confounded over time. For example, a doctor might want to know the effect of a treatment  given at multiple times whose status depends on the health of the patient at a given time.  One way to get an unbiased estimate of the causal effect is to use G-computation. The basic  steps for using G-computation in CausalELM are below.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nFor a good overview of G-Computation see:Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé \nRousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann \nFoucher. \"G-computation, propensity score-based methods, and targeted maximum likelihood \nestimator for causal inference with different covariates sets: a comparative simulation \nstudy.\" Scientific reports 10, no. 1 (2020): 9219.","category":"page"},{"location":"guide/gcomputation/#Step-1:-Initialize-a-Model","page":"G-computation","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"The GComputation constructor takes at least three arguments: covariates, treatment statuses,  outcomes, all of which can be either an array or any data structure that implements the  Tables.jl interface (e.g. DataFrames). This implementation supports binary treatments and  binary, continuous, time to event, and count outcome variables.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nNon-binary categorical outcomes are treated as continuous.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"tip: Tip\nYou can also specify the causal estimand, which activation function to use, whether the  data is of a temporal nature, the number of extreme learning machines to use, the  number of features to consider for each extreme learning machine, the number of  bootstrapped observations to include in each extreme learning machine, and the number of  neurons to use during estimation. These options are specified with the following keyword  arguments: quantity_of_interest, activation, temporal, num_machines, num_feats,  sample_size, and num_neurons.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Create some data with a binary treatment\nX, T, Y =  rand(1000, 5), [rand()<0.4 for i in 1:1000], rand(1000)\n\n# We could also use DataFrames or any other package that implements the Tables.jl API\n# using DataFrames\n# X = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:1000]), DataFrame(y=rand(1000))\ng_computer = GComputation(X, T, Y)","category":"page"},{"location":"guide/gcomputation/#Step-2:-Estimate-the-Causal-Effect","page":"G-computation","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"To estimate the causal effect, we pass the model above to estimatecausaleffect!.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Note that we could also estimate the ATT by setting quantity_of_interest=\"ATT\"\nestimate_causal_effect!(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-3:-Get-a-Summary","page":"G-computation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We can get a summary of the model by pasing the model to the summarize method.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"!!!note     To calculate the p-value and standard error for the treatment effect, you can set the      inference argument to false. However, p-values and standard errors are calculated via      randomization inference, which will take a long time. But can be sped up by launching      Julia with a higher number of threads.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"summarize(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-4:-Validate-the-Model","page":"G-computation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we simulate counterfactual outcomes that are  different from the observed outcomes, estimate models with the simulated counterfactual  outcomes, and take the averages. If the outcome is continuous, the noise for the simulated  counterfactuals is drawn from N(0, dev) for each element in devs and each outcome,  multiplied by the original outcome, and added to the original outcome. For discrete  variables, each outcome is replaced with a different value in the range of outcomes with  probability ϵ for each ϵ in devs, otherwise the default is 0.025, 0.05, 0.075, 0.1. If the  average estimate for a given level of violation differs greatly from the effect estimated on  the actual data, then the model is very sensitive to violations of the counterfactual  consistency assumption for that level of violation. Next, this method tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"tip: Tip\nOne can also specify the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for G-computation.  If the assumptions are not met then any estimates may be biased and lead to incorrect  conclusions.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"validate(g_computer)","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All contributions are welcome. To ensure contributions align with the existing code base and  are not duplicated, please follow the guidelines below.","category":"page"},{"location":"contributing/#Reporting-a-Bug","page":"Contributing","title":"Reporting a Bug","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To report a bug, open an issue on the CausalELM GitHub page. Please include all relevant information, such as what methods were called, the operating system used, the  verion/s of causalELM used, the verion/s of Julia used, any tracebacks or error codes, and  any other information that would be helpful for debugging. Also be sure to use the bug label.","category":"page"},{"location":"contributing/#Requesting-New-Features","page":"Contributing","title":"Requesting New Features","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before requesting a new feature, please check the issues page on GitHub to make sure someone else did not already request the same feature. If this is not the case, then please  open an issue that explains what function or method you would like to be added and how you  believe it should behave. Also be sure to use the enhancement tag.","category":"page"},{"location":"contributing/#Contributing-Code","page":"Contributing","title":"Contributing Code","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before submitting a pull request, please  open an issue explaining what the proposed code is and why you want to add it, if there is  not already an issue that addresses your changes and you are not fixing something very  minor. When submitting a pull request, please reference the relevant issue/s and ensure your  code follows the guidelines below.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before being merged, all pull requests should be well tested and all tests must be passing.\nAll abstract types, structs, functions, methods, macros, and constants have docstrings    that follow the same format as the other docstrings. These functions should also be    included in the relevant section of the API Manual.\nMost new structs for estimating causal effects should have mostly the same fields. To    reduce the burden of repeatedly defining all these fields, it is advisable to use the    modelconfig and standardinput_data macros to programmatically generate fields for new    structs. Doing so will ensure that with little to no effort the new structs will work    with the summarize and validate methods.\nThere are no repeated code blocks. If there are repeated codeblocks, then they should be    consolidated into a separate function.\nInteranl methods can contain types and be parametric but public methods should be as    general as possible.\nMinimize use of new constants and macros. If they must be included, the reason for their    inclusion should be obvious or included in the docstring.\nAvoid using global variables and constants.\nCode should take advantage of Julia's built in macros for performance. Use @inbounds,    @view, @fastmath, and @simd when possible.\nWhen appending to an array in a loop, preallocate the array and update its values by    index.\nAvoid long functions and decompose them into smaller functions or methods. A general    rule is that function definitions should fit within the screen of a laptop.\nUse self-explanatory names for variables, methods, structs, constants, and macros.\nMake generous use of whitespace.\nAll functions should include docstrings.\n**  Docstrings may contain arguments, keywords, notes, references, and examples sections        in that order but some sections may be skipped.\n**  At a minimum, docstrings should contain the signature/s, a short description, and        examples\n**  Each section should include its own level one header.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nCausalELM follows the Blue style guide and all code is automatically formatted to  conform with this standard upon being pushed to GitHub.","category":"page"},{"location":"contributing/#Updating-or-Fixing-Documentation","page":"Contributing","title":"Updating or Fixing Documentation","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To propose a change to the documentation please submit an issue  or pull request.","category":"page"},{"location":"guide/doublemachinelearning/#Double-Machine-Learning","page":"Double Machine Learning","title":"Double Machine Learning","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"Double machine learning, also called debiased or orthogonalized machine learning, enables estimating causal effects when the dimensionality of the covariates is too high for linear  regression or the treatment or outcomes cannot be easily modeled parametrically. Double  machine learning estimates models of the treatment assignment and outcome and then combines  them in a final model. This is a semiparametric model in the sense that the first stage  models can take on any functional form but the final stage model is a linear combination of  the residuals from the first stage models.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nFor more information see:Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,  Whitney Newey, and James Robins. \"Double/debiased machine learning for treatment and  structural parameters.\" (2018): C1-C68.","category":"page"},{"location":"guide/doublemachinelearning/#Step-1:-Initialize-a-Model","page":"Double Machine Learning","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"The DoubleMachineLearning constructor takes at least three arguments—covariates, a  treatment statuses, and outcomes, all of which may be either an array or any struct that  implements the Tables.jl interface (e.g. DataFrames). This estimator supports binary, count,  or continuous treatments and binary, count, continuous, or time to event outcomes.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nNon-binary categorical outcomes are treated as continuous.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"tip: Tip\nYou can also specify the the number of folds to use for cross-fitting, the number of  extreme learning machines to incorporate in the ensemble, the number of features to  consider for each extreme learning machine, the activation function to use, the number  of observations to bootstrap in each extreme learning machine, and the number of neurons  in each extreme learning machine. These arguments are specified with the folds,  num_machines, num_features, activation, sample_size, and num_neurons keywords.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Create some data with a binary treatment\nX, T, Y, W = rand(100, 5), [rand()<0.4 for i in 1:100], rand(100), rand(100, 4)\n\n# We could also use DataFrames or any other package implementing the Tables.jl API\n# using DataFrames\n# X = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100), x5=rand(100))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:100]), DataFrame(y=rand(100))\ndml = DoubleMachineLearning(X, T, Y)","category":"page"},{"location":"guide/doublemachinelearning/#Step-2:-Estimate-the-Causal-Effect","page":"Double Machine Learning","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"To estimate the causal effect, we call estimatecausaleffect! on the model above.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# we could also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimate_causal_effect!(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Get-a-Summary","page":"Double Machine Learning","title":"Get a Summary","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can get a summary of the model by pasing the model to the summarize method.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"!!!note     To calculate the p-value and standard error for the treatmetn effect, you can set the      inference argument to false. However, p-values and standard errors are calculated via      randomization inference, which will take a long time. But can be sped up by launching      Julia with a higher number of threads.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Can also use the British spelling\n# summarise(dml)\nsummarize(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Step-4:-Validate-the-Model","page":"Double Machine Learning","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we simulate counterfactual outcomes that are  different from the observed outcomes, estimate models with the simulated counterfactual  outcomes, and take the averages. If the outcome is continuous, the noise for the simulated  counterfactuals is drawn from N(0, dev) for each element in devs and each outcome,  multiplied by the original outcome, and added to the original outcome. For discrete  variables, each outcome is replaced with a different value in the range of outcomes with  probability ϵ for each ϵ in devs, otherwise the default is 0.025, 0.05, 0.075, 0.1. If the  average estimate for a given level of violation differs greatly from the effect estimated on  the actual data, then the model is very sensitive to violations of the counterfactual  consistency assumption for that level of violation. Next, this method tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"tip: Tip\nOne can also specify the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for double machine  learning. If the assumptions are not met then any estimates may be biased and lead to  incorrect conclusions.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"validate(g_computer)","category":"page"},{"location":"guide/its/#Interrupted-Time-Series-Analysis","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Analysis","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Sometimes we want to know how an outcome variable for a single unit changed after an event  or intervention. For example, if regulators announce sanctions against company A, we might  want to know how the price of company A's stock changed after the announcement. Since we do  not know what the price of Company A's stock would have been if the santions were not  announced, we need some way to predict those values. An interrupted time series analysis  does this by using some covariates that are related to the outcome but not related to  whether the event happened to predict what would have happened. The estimated effects are  the differences between the predicted post-event counterfactual outcomes and the observed  post-event outcomes, which can also be aggregated to mean or cumulative effects.  Estimating an interrupted time series design in CausalELM consists of three steps.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nFor a general overview of interrupted time series estimation see:Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. \"Interrupted time series \nregression for the evaluation of public health interventions: a tutorial.\" International \njournal of epidemiology 46, no. 1 (2017): 348-355.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nThe flavor of interrupted time series implemented here is similar to the variant proposed  in:Brodersen, Kay H., Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L. Scott. \n\"Inferring causal impact using Bayesian structural time-series models.\" (2015): 247-274.in that, although it is not Bayesian, it uses a nonparametric model of the pre-treatment  period and uses that model to forecast the counterfactual in the post-treatment period, as  opposed to the commonly used segment linear regression.","category":"page"},{"location":"guide/its/#Step-1:-Initialize-an-interrupted-time-series-estimator","page":"Interrupted Time Series Estimation","title":"Step 1: Initialize an interrupted time series estimator","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"The InterruptedTimeSeries constructor takes at least four agruments: pre-event covariates,  pre-event outcomes, post-event covariates, and post-event outcomes, all of which can be  either an array or any data structure that implements the Tables.jl interface (e.g.  DataFrames). The interrupted time series estimator assumes outcomes are either continuous,  count, or time to event variables.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nNon-binary categorical outcomes are treated as continuous.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"tip: Tip\nYou can also specify which activation function to use, the number of extreme learning  machines to use, the number of features to consider for each extreme learning machine,  the number of bootstrapped observations to include in each extreme learning machine, and  the number of neurons to use during estimation. These options are specified with the  following keyword arguments: activation, num_machines, num_feats, sample_size,  and num_neurons.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"# Generate some data to use\nX₀, Y₀, X₁, Y₁ =  rand(1000, 5), rand(1000), rand(100, 5), rand(100)\n\n# We could also use DataFrames or any other package that implements the Tables.jl interface\n# using DataFrames\n# X₀ = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# X₁ = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# Y₀, Y₁ = DataFrame(y=rand(1000)), DataFrame(y=rand(1000))\nits = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)","category":"page"},{"location":"guide/its/#Step-2:-Estimate-the-Treatment-Effect","page":"Interrupted Time Series Estimation","title":"Step 2: Estimate the Treatment Effect","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Estimating the treatment effect only requires one argument: an InterruptedTimeSeries struct.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"estimate_causal_effect!(its)","category":"page"},{"location":"guide/its/#Step-3:-Get-a-Summary","page":"Interrupted Time Series Estimation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"We can get a summary of the model by pasing the model to the summarize method.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"!!!note     To calculate the p-value and standard error for the treatment effect, you can set the      inference argument to false. However, p-values and standard errors are calculated via      randomization inference, which will take a long time. But can be sped up by launching      Julia with a higher number of threads.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"summarize(its)","category":"page"},{"location":"guide/its/#Step-4:-Validate-the-Model","page":"Interrupted Time Series Estimation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"For an interrupted time series design to work well we need to be able to get an unbiased  prediction of the counterfactual outcomes. If the event or intervention effected the  covariates we are using to predict the counterfactual outcomes, then we will not be able to  get unbiased predictions. We can verify this by conducting a Chow Test on the covariates. An ITS design also assumes that any observed effect is due to the hypothesized intervention,  rather than any simultaneous interventions, anticipation of the intervention, or any  intervention that ocurred after the hypothesized intervention. We can use a Wald supremum  test to see if the hypothesized intervention ocurred where there is the largest structural  break in the outcome or if there was a larger, statistically significant break in the  outcome that could confound an ITS analysis. The covariates in an ITS analysis should be  good predictors of the outcome. If this is the case, then adding irrelevant predictors  should not have much of a change on the results of the analysis. We can conduct all these  tests in one line of code.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"tip: Tip\nOne can also specify the number of simulated confounders to generate to test the sensitivity  of the model to confounding and the minimum and maximum proportion of data to use in the  Wald supremum test by including the n, low, and high keyword arguments.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for interrupted time  series estimation. If the assumptions are not met then any estimates may be biased and  lead to incorrect conclusions.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nFor a review of interrupted time series identifying assumptions and robustness checks, see:Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single \ninterrupted time series design. No. w26080. National Bureau of Economic Research, 2019.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"validate(its)","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"<div style=\"width:100%; height:15px;;\n        border-radius:6px;text-align:center;\n        color:#1e1e20\">\n    <a class=\"github-button\" href=\"https://github.com/dscolby/CausalELM.jl\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star dscolby/CausalELM.jl on GitHub\" style=\"margin:auto\">Star</a>\n    <script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n</div>","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"#Overview","page":"CausalELM","title":"Overview","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM leverages new techniques in machine learning and statistics to estimate individual  and aggregate treatment effects in situations where traditional methods are unsatisfactory  or infeasible. To enable this, CausalELM provides a simple API to initialize a model,  estimate a causal effect, get a summary of the model, and test its robustness. CausalELM  includes estimators for interupted time series analysis, G-Computation, double machine  learning, S-Learning, T-Learning, X-Learning, R-learning, and doubly robust estimation.  Underlying all these estimators are bagged extreme learning machines. Extreme learning  machines are a single layer feedfoward neural network that relies on randomized weights and  least squares optimization, making them expressive, simple, and computationally  efficient. Combining them with bagging reduces the variance caused by the randomization of  weights and provides a form of regularization that does not have to be tuned through cross  validation. These attributes make CausalELM a very simple and powerful package for  estimating treatment effects.","category":"page"},{"location":"#Features","page":"CausalELM","title":"Features","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Estimate a causal effect, get a summary, and validate assumptions in just four lines of code\nBagging improves performance and reduces variance without the need to tune a regularization parameter\nEnables using the same structs for regression and classification\nIncludes 13 activation functions and allows user-defined activation functions\nMost inference and validation tests do not assume functional or distributional forms\nImplements the latest techniques from statistics, econometrics, and biostatistics\nWorks out of the box with arrays or any data structure that implements the Tables.jl interface\nCodebase is high-quality, well tested, and regularly updated","category":"page"},{"location":"#What's-New?","page":"CausalELM","title":"What's New?","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Now includes doubly robust estimator for CATE estimation\nAll estimators now implement bagging to reduce predictive performance and reduce variance\nCounterfactual consistency validation simulates more realistic violations of the counterfactual consistency assumption\nUses a simple heuristic to choose the number of neurons, which reduces training time and still works well in practice\nProbability clipping for classifier predictions and residuals is no longer necessary due to the bagging procedure\nCausalELM talk has been accepted to JuliaCon 2024!","category":"page"},{"location":"#What-makes-CausalELM-different?","page":"CausalELM","title":"What makes CausalELM different?","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Other packages, mainly EconML, DoWhy, CausalAI, and CausalML, have similar funcitonality.  Beides being written in Julia rather than Python, the main differences between CausalELM and  these libraries are:","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Simplicity is core to casualELM's design philosophy. CausalELM only uses one type of   machine learning model, extreme learning machines (with bagging) and does not require    you to import any other packages or initialize machine learning models, pass machine    learning structs to CausalELM's estimators, convert dataframes or arrays to a special    type, or one hot encode categorical treatments. By trading a little bit of flexibility    for a simpler API, all of CausalELM's functionality can be used with just four lines of    code.\nAs part of this design principle, CausalELM's estimators decide whether to use regression    or classification based on the type of outcome variable. This is in contrast to most    machine learning packages, which have separate classes or structs fro regressors and    classifiers of the same model.\nCausalELM's validate method, which is specific to each estimator, allows you to validate    or test the sentitivity of an estimator to possible violations of identifying assumptions.\nUnlike packages that do not allow you to estimate p-values and standard errors, use    bootstrapping to estimate them, or use incorrect hypothesis tests, all of CausalELM's    estimators provide p-values and standard errors generated via approximate randomization    inference. \nCausalELM strives to be lightweight while still being powerful and therefore does not    have external dependencies: all the functions it uses are in the Julia standard library.\nThe other packages and many others mostly use techniques from one field. Instead,    CausalELM incorporates a hodgepodge of ideas from statistics, machine learning,    econometrics, and biostatistics.","category":"page"},{"location":"#Installation","page":"CausalELM","title":"Installation","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM requires Julia version 1.7 or greater and can be installed from the REPL as shown  below. ","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"using Pkg \nPkg.add(\"CausalELM\")","category":"page"},{"location":"guide/metalearners/#Metalearners","page":"Metalearners","title":"Metalearners","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Instead of knowing the average causal effect, we might want to know which units benefit and  which units lose by being exposed to a treatment. For example, a cash transfer program might  motivate some people to work harder and incentivize others to work less. Thus, we might want  to know how the cash transfer program affects individuals instead of it average affect on  the population. To do so, we can use metalearners. Depending on the scenario, we may want to  use an S-learner, T-learner, X-learner, R-learner, or doubly robust learner. The basic steps  to use all five metalearners are below. The difference between the metalearners is how they  estimate the CATE and what types of variables they can handle. In the case of S, T, X, and  doubly robust learners, they can only handle binary treatments. On the other hand,  R-learners can handle binary, categorical, count, or continuous treatments but only supports  continuous outcomes.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nFor a deeper dive on S-learning, T-learning, and X-learning see:Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.To learn more about R-learning see:Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment \neffects.\" Biometrika 108, no. 2 (2021): 299-319.To see the details out doubly robust estimation implemented in CausalELM see:Kennedy, Edward H. \"Towards optimal doubly robust estimation of heterogeneous causal \neffects.\" Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.","category":"page"},{"location":"guide/metalearners/#Initialize-a-Metalearner","page":"Metalearners","title":"Initialize a Metalearner","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"S-learners, T-learners, X-learners, R-learners, and doubly robust estimators all take at  least three arguments—covariates, treatment statuses, and outcomes, all of which can be  either an array or any struct that implements the Tables.jl interface (e.g. DataFrames). S,  T, X, and doubly robust learners support binary treatment variables and binary, continuous,  count, or time to event outcomes. The R-learning estimator supports binary, continuous, or  count treatment variables and binary, continuous, count, or time to event outcomes.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nNon-binary categorical outcomes are treated as continuous.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"tip: Tip\nYou can also specify the the number of folds to use for cross-fitting, the number of  extreme learning machines to incorporate in the ensemble, the number of features to  consider for each extreme learning machine, the activation function to use, the number  of observations to bootstrap in each extreme learning machine, and the number of neurons  in each extreme learning machine. These arguments are specified with the folds,  num_machines, num_features, activation, sample_size, and num_neurons keywords.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"# Generate data to use\nX, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]\n\n# We could also use DataFrames or any other package that implements the Tables.jl API\n# using DataFrames\n# X = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:1000]), DataFrame(y=rand(1000))\ns_learner = SLearner(X, Y, T)\nt_learner = TLearner(X, Y, T)\nx_learner = XLearner(X, Y, T)\nr_learner = RLearner(X, Y, T)\ndr_learner = DoublyRobustLearner(X, T, Y)","category":"page"},{"location":"guide/metalearners/#Estimate-the-CATE","page":"Metalearners","title":"Estimate the CATE","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can estimate the CATE for all the models by passing them to estimatecausaleffect!.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"estimate_causal_effect!(s_learner)\nestimate_causal_effect!(t_learner)\nestimate_causal_effect!(x_learner)\nestimate_causal_effect!(r_learner)\nestimate_causal_effect!(dr_lwarner)","category":"page"},{"location":"guide/metalearners/#Get-a-Summary","page":"Metalearners","title":"Get a Summary","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can get a summary of the model by pasing the model to the summarize method.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"!!!note     To calculate the p-value and standard error for the treatment effect, you can set the      inference argument to false. However, p-values and standard errors are calculated via      randomization inference, which will take a long time. But can be sped up by launching      Julia with a higher number of threads.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"summarize(s_learner)\nsummarize(t_learner)\nsummarize(x_learner)\nsummarize(r_learner)\nsummarize(dr_learner)","category":"page"},{"location":"guide/metalearners/#Step-4:-Validate-the-Model","page":"Metalearners","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we simulate counterfactual outcomes that are  different from the observed outcomes, estimate models with the simulated counterfactual  outcomes, and take the averages. If the outcome is continuous, the noise for the simulated  counterfactuals is drawn from N(0, dev) for each element in devs and each outcome,  multiplied by the original outcome, and added to the original outcome. For discrete  variables, each outcome is replaced with a different value in the range of outcomes with  probability ϵ for each ϵ in devs, otherwise the default is 0.025, 0.05, 0.075, 0.1. If the  average estimate for a given level of violation differs greatly from the effect estimated on  the actual data, then the model is very sensitive to violations of the counterfactual  consistency assumption for that level of violation. Next, this method tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"tip: Tip\nOne can also specify the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for interrupted time  series estimation. If the assumptions are not met then any estimates may be biased and  lead to incorrect conclusions.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"validate(s_learner)\nvalidate(t_learner)\nvalidate(x_learner)\nvalidate(r_learner)\nvalidate(dr_learner)","category":"page"}]
}
