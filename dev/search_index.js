var documenterSearchIndex = {"docs":
[{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"These release notes adhere to the keep a changelog format. Below is a list of changes since CausalELM was first released.","category":"page"},{"location":"release_notes/#Version-[v0.6.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.6.0)-2024-03-23","page":"Release Notes","title":"Version v0.6.0 - 2024-03-23","text":"","category":"section"},{"location":"release_notes/#Added","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Implemented doubly robust learner for CATE estimation #31\nProvided better explanations of supported treatment and outcome variable types in the docs #41\nAdded support for specifying confounders, W, separate from covariates of interest, X, for double machine ","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"learning and doubly robust estimation 39","category":"page"},{"location":"release_notes/#Changed","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Removed the estimatecausaleffect! call in the model constructor docstrings #35\nStandardized and improved docstrings and added doctests #44\nCounterfactual consistency now simulates outcomes that violate the counterfactual consistency assumption rather than ","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"binning of treatments and works with discrete or continuous treatments #33","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Refactored estimator and metalearner structs, constructors, and estimatecausaleffect! methods #45","category":"page"},{"location":"release_notes/#Fixed","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Clipped probabilities between 0 and 1 for estimators that use predictions of binary variables #36\nFixed sample splitting and cross fitting procedure for doubly robust estimation #42\nAddressed numerical instability when finding the ridge penalty by replacing the previous ridge formula with ","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"generalized cross validation #43","category":"page"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Uses the correct variable in the ommited predictor test for interrupted time series.\nUses correct range for p-values in interrupted time series validation tests.\nCorrectly subsets the data for ATT estimation in G-computation #52","category":"page"},{"location":"release_notes/#Version-[v0.5.1](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.5.1)-2024-01-15","page":"Release Notes","title":"Version v0.5.1 - 2024-01-15","text":"","category":"section"},{"location":"release_notes/#Added-2","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"More descriptive docstrings #21","category":"page"},{"location":"release_notes/#Fixed-2","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Permutation of continuous treatments draws from a continuous, instead of discrete uniform distribution during randomization inference","category":"page"},{"location":"release_notes/#Version-[v0.5.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.5.0)-2024-01-13","page":"Release Notes","title":"Version v0.5.0 - 2024-01-13","text":"","category":"section"},{"location":"release_notes/#Added-3","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Constructors for estimators taht accept dataframes from DataFrames.jl #25","category":"page"},{"location":"release_notes/#Changed-2","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Estimators can handle any array whose values are <:Real #23\nEstimator constructors are now called with model(X, T, Y) instead of model(X, Y, T)\nRemoved excess type constraints for many methods #23\nVectorized a few for loops\nIncreased test coverage","category":"page"},{"location":"release_notes/#Version-[v0.4.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.4.0)-2024-01-06","page":"Release Notes","title":"Version v0.4.0 - 2024-01-06","text":"","category":"section"},{"location":"release_notes/#Added-4","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"R-learning\nSoftmax function for arrays","category":"page"},{"location":"release_notes/#Changed-3","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Moved all types and methods under the main module\nDecreased size of function definitions #22\nSLearner has a G-computation field that does the heavy lifting for S-learning\nRemoved excess fields from estimator structs","category":"page"},{"location":"release_notes/#Fixed-3","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Changed the incorrect name of DoublyRobustEstimation struct to DoubleMachineLearning\nCaclulation of risk ratios and E-values\nCalculation of validation metrics for multiclass classification\nCalculation of output weights for L2 regularized extreme learning machines","category":"page"},{"location":"release_notes/#Version-[v0.3.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.3.0)-2023-11-25","page":"Release Notes","title":"Version v0.3.0 - 2023-11-25","text":"","category":"section"},{"location":"release_notes/#Added-5","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Splitting of temporal data for cross validation 18\nMethods to validate/test senstivity to violations of identifying assumptions #16","category":"page"},{"location":"release_notes/#Changed-4","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Converted all functions and methods to snake case #17\nRandomization inference for interrupted time series randomizes all the indices #15","category":"page"},{"location":"release_notes/#Fixed-4","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Issue related to recoding variables to calculate validation metrics for cross validation","category":"page"},{"location":"release_notes/#Version-[v0.2.1](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.2.1)-2023-06-07","page":"Release Notes","title":"Version v0.2.1 - 2023-06-07","text":"","category":"section"},{"location":"release_notes/#Added-6","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Cross fitting to the doubly robust estimator","category":"page"},{"location":"release_notes/#Version-[v0.2.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.2.0)-2023-04-16","page":"Release Notes","title":"Version v0.2.0 - 2023-04-16","text":"","category":"section"},{"location":"release_notes/#Added-7","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Calculation of p-values and standard errors via randomization inference","category":"page"},{"location":"release_notes/#Changed-5","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Divided package into modules","category":"page"},{"location":"release_notes/#Version-[v0.1.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.1.0)-2023-02-14","page":"Release Notes","title":"Version v0.1.0 - 2023-02-14","text":"","category":"section"},{"location":"release_notes/#Added-8","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Event study, g-computation, and doubly robust estimators\nS-learning, T-learning, and X-learning\nModel summarization methods","category":"page"},{"location":"api/#CausalELM","page":"API","title":"CausalELM","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Most of the methods and structs here are private, not exported, should not be called by the  user, and are documented for the purpose of developing CausalELM or to facilitate  understanding of the implementation.","category":"page"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"InterruptedTimeSeries\nGComputation\nDoubleMachineLearning\nSLearner\nTLearner\nXLearner\nRLearner\nDoublyRobustLearner\nCausalELM.CausalEstimator\nCausalELM.Metalearner\nCausalELM.ExtremeLearningMachine\nCausalELM.ExtremeLearner\nCausalELM.RegularizedExtremeLearner\nCausalELM.Nonbinary\nCausalELM.Binary\nCausalELM.Count\nCausalELM.Continuous","category":"page"},{"location":"api/#CausalELM.InterruptedTimeSeries","page":"API","title":"CausalELM.InterruptedTimeSeries","text":"InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; kwargs...)\n\nInitialize an interrupted time series estimator. \n\nArguments\n\nX₀::Any: an array or DataFrame of covariates from the pre-treatment period.\nY₁::Any: an array or DataFrame of outcomes from the pre-treatment period.\nX₁::Any: an array or DataFrame of covariates from the post-treatment period.\nY₁::Any: an array or DataFrame of outcomes from the post-treatment period.\nregularized::Function=true: whether to use L2 regularization\n\nKeywords\n\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross validation.\nmin_neurons::Real: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Real: the maximum number of neurons to consider for the extreme learner.\nfolds::Real: the number of cross validation folds to find the best number of neurons.\niterations::Real: the number of iterations to perform cross validation between    minneurons and maxneurons.\napproximator_neurons::Real: the number of nuerons in the validation loss approximator    network.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor a simple linear regression-based tutorial on interrupted time series analysis see:     Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. \"Interrupted time series      regression for the evaluation of public health interventions: a tutorial.\" International      journal of epidemiology 46, no. 1 (2017): 348-355.\n\nFor details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a      method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979): 215-223.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> m2 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; regularized=false)\njulia> x₀_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100))\njulia> y₀_df = DataFrame(y=rand(100))\njulia> x₁_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100)) \njulia> y₁_df = DataFrame(y=rand(100))\njulia> m3 = InterruptedTimeSeries(x₀_df, y₀_df, x₁_df, y₁_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.GComputation","page":"API","title":"CausalELM.GComputation","text":"GComputation(X, T, Y; kwargs...)\n\nInitialize a G-Computation estimator.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nquantity_of_interest::String: ATE for average treatment effect or ATT for average    treatment effect on the treated.\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross    validation.\n`min_neurons::Real: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Real: the maximum number of neurons to consider for the extreme learner.\nfolds::Real: the number of cross validation folds to find the best number of neurons.\niterations::Real: the number of iterations to perform cross validation between    minneurons and maxneurons.\napproximator_neurons::Real: the number of nuerons in the validation loss approximator    network.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor a good overview of G-Computation see:     Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé      Rousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann      Foucher. \"G-computation, propensity score-based methods, and targeted maximum likelihood      estimator for causal inference with different covariates sets: a comparative simulation      study.\" Scientific reports 10, no. 1 (2020): 9219.\n\nFor details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a      method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979): 215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, T, Y)\njulia> m2 = GComputation(X, T, Y; task=\"regression\")\njulia> m3 = GComputation(X, T, Y; task=\"regression\", quantity_of_interest=\"ATE)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100)) \njulia> m5 = GComputation(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.DoubleMachineLearning","page":"API","title":"CausalELM.DoubleMachineLearning","text":"DoubleMachineLearning(X, T, Y; kwargs...)\n\nInitialize a double machine learning estimator with cross fitting.\n\nArguments\n\nX::Any: an array or DataFrame of covariates of interest.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nW::Any: an array or dataframe of all possible confounders.\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross validation.\nmin_neurons::Real: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Real: the maximum number of neurons to consider for the extreme learner.\nfolds::Real: the number of cross validation folds to find the best number of neurons.\niterations::Real: the number of iterations to perform cross validation between    minneurons and maxneurons.\napproximator_neurons::Real: the number of nuerons in the validation loss approximator    network.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.\n\nUnlike other estimators, this method does not support time series or panel data. This method  also does not work as well with smaller datasets because it estimates separate outcome  models for the treatment and control groups.\n\nReferences\n\nFor more information see:     Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,      Whitney Newey, and James Robins. \"Double/debiased machine learning for treatment and      structural parameters.\" (2016): C1-C68.\n\nFor details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a      method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979): 215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> m2 = DoubleMachineLearning(X, T, Y; task=\"regression\")\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m3 = DoubleMachineLearning(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.SLearner","page":"API","title":"CausalELM.SLearner","text":"SLearner(X, T, Y; kwargs...)\n\nInitialize a S-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross validation.\nmin_neurons::Real: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Real: the maximum number of neurons to consider for the extreme learner.\nfolds::Real: the number of cross validation folds to find the best number of neurons.\niterations::Real: the number of iterations to perform cross validation between \n\nminneurons and maxneurons.\n\napproximator_neurons::Real: the number of nuerons in the validation loss approximator \n\nnetwork.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor an overview of S-Learners and other metalearners see: Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for  estimating heterogeneous treatment effects using machine learning.\" Proceedings of  the national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nFor details and a derivation of the generalized cross validation estimator see: Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a  method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979):  215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = SLearner(X, T, Y)\njulia> m2 = SLearner(X, T, Y; task=\"regression\")\njulia> m3 = SLearner(X, T, Y; task=\"regression\", regularized=true)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m4 = SLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.TLearner","page":"API","title":"CausalELM.TLearner","text":"TLearner(X, T, Y; kwargs...)\n\nInitialize a T-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross \n\nvalidation.\n\nmin_neurons::Real: the minimum number of neurons to consider for the extreme \n\nlearner.\n\nmax_neurons::Real: the maximum number of neurons to consider for the extreme \n\nlearner.\n\nfolds::Real: the number of cross validation folds to find the best number of \n\nneurons.\n\niterations::Real: the number of iterations to perform cross validation between \n\nminneurons and maxneurons.\n\napproximator_neurons::Real: the number of nuerons in the validation loss approximator \n\nnetwork.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor an overview of T-Learners and other metalearners see: Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for  estimating heterogeneous treatment effects using machine learning.\" Proceedings of  the national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nFor details and a derivation of the generalized cross validation estimator see: Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a  method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979):  215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = TLearner(X, T, Y)\njulia> m2 = TLearner(X, T, Y; task=\"regression\")\njulia> m3 = TLearner(X, T, Y; task=\"regression\", regularized=true)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m4 = TLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.XLearner","page":"API","title":"CausalELM.XLearner","text":"XLearner(X, T, Y; kwargs...)\n\nInitialize an X-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross \n\nvalidation.\n\nmin_neurons::Real: the minimum number of neurons to consider for the extreme \n\nlearner.\n\nmax_neurons::Real: the maximum number of neurons to consider for the extreme \n\nlearner.\n\nfolds::Real: the number of cross validation folds to find the best number of \n\nneurons.\n\niterations::Real: the number of iterations to perform cross validation between \n\nminneurons and maxneurons.\n\napproximator_neurons::Real: the number of nuerons in the validation loss \n\napproximator network.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor an overview of X-Learners and other metalearners see: Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for  estimating heterogeneous treatment effects using machine learning.\" Proceedings of  the national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nFor details and a derivation of the generalized cross validation estimator see: Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a  method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979):  215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> m2 = XLearner(X, T, Y; task=\"regression\")\njulia> m3 = XLearner(X, T, Y; task=\"regression\", regularized=true)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m4 = XLearner(x_df, t_df, y_df)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.RLearner","page":"API","title":"CausalELM.RLearner","text":"RLearner(X, T, Y; kwargs...)\n\nInitialize an R-Learner.\n\nArguments\n\nX::Any: an array or DataFrame of covariates of interest.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nW::Any : an array of all possible confounders.\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross validation.\nmin_neurons::Real: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Real: the maximum number of neurons to consider for the extreme learner.\nfolds::Real: the number of cross validation folds to find the best number of neurons.\niterations::Real: the number of iterations to perform cross validation between    minneurons and maxneurons.\napproximator_neurons::Real: the number of nuerons in the validation loss approximator    network.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor an explanation of R-Learner estimation see:     Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment      effects.\" Biometrika 108, no. 2 (2021): 299-319.\n\nFor details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a      method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979): 215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = RLearner(X, T, Y)\njulia> m2 = RLearner(X, T, Y; t_cat=true)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m4 = RLearner(x_df, t_df, y_df)\n\njulia> w = rand(100, 6)\njulia> m5 = RLearner(X, T, Y, W=w)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.DoublyRobustLearner","page":"API","title":"CausalELM.DoublyRobustLearner","text":"DoublyRobustLearner(X, T, Y; kwargs...)\n\nInitialize a doubly robust CATE estimator.\n\nArguments\n\nX::Any: an array or DataFrame of covariates of interest.\nT::Any: an vector or DataFrame of treatment statuses.\nY::Any: an array or DataFrame of outcomes.\n\nKeywords\n\nW::Any: an array or dataframe of all possible confounders.\nregularized::Function=true: whether to use L2 regularization\nactivation::Function=relu: the activation function to use.\nvalidation_metric::Function: the validation metric to calculate during cross validation.\nmin_neurons::Real: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Real: the maximum number of neurons to consider for the extreme learner.\nfolds::Real: the number of cross validation folds to find the best number of neurons.\niterations::Real: the number of iterations to perform cross validation between    minneurons and maxneurons.\napproximator_neurons::Real: the number of nuerons in the validation loss approximator    network.\n\nNotes\n\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.\n\nReferences\n\nFor an explanation of doubly robust cate estimation see:     Kennedy, Edward H. \"Towards optimal doubly robust estimation of heterogeneous causal      effects.\" Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.\n\nFor details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a      method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979): 215-223.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoublyRobustLearner(X, T, Y)\njulia> m2 = DoublyRobustLearnerLearner(X, T, Y; t_cat=true)\n\njulia> x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))\njulia> t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))\njulia> m4 = DoublyRobustLearner(x_df, t_df, y_df)\n\njulia> w = rand(100, 6)\njulia> m5 = DoublyRobustLearner(X, T, Y, W=w)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.CausalEstimator","page":"API","title":"CausalELM.CausalEstimator","text":"Abstract type for GComputation and DoubleMachineLearning\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Metalearner","page":"API","title":"CausalELM.Metalearner","text":"Abstract type for metalearners\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.ExtremeLearningMachine","page":"API","title":"CausalELM.ExtremeLearningMachine","text":"Abstract type that includes vanilla and L2 regularized Extreme Learning Machines\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.ExtremeLearner","page":"API","title":"CausalELM.ExtremeLearner","text":"ExtremeLearner(X, Y, hidden_neurons, activation)\n\nConstruct an ExtremeLearner for fitting and prediction.\n\nNotes\n\nWhile it is possible to use an ExtremeLearner for regression, it is recommended to use  RegularizedExtremeLearner, which imposes an L2 penalty, to reduce multicollinearity.\n\nReferences\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nSee also 'RegularizedExtremeLearner'.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.RegularizedExtremeLearner","page":"API","title":"CausalELM.RegularizedExtremeLearner","text":"RegularizedExtremeLearner(X, Y, hidden_neurons, activation)\n\nConstruct a RegularizedExtremeLearner for fitting and prediction.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Nonbinary","page":"API","title":"CausalELM.Nonbinary","text":"Abstract type used to dispatch risk_ratio on nonbinary treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Binary","page":"API","title":"CausalELM.Binary","text":"Type used to dispatch risk_ratio on binary treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Count","page":"API","title":"CausalELM.Count","text":"Type used to dispatch risk_ratio on count treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Continuous","page":"API","title":"CausalELM.Continuous","text":"Type used to dispatch risk_ratio on continuous treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#Activation-Functions","page":"API","title":"Activation Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"binary_step\nσ\ntanh\nrelu\nleaky_relu\nswish\nsoftmax\nsoftplus\ngelu\ngaussian\nhard_tanh\nelish\nfourier","category":"page"},{"location":"api/#CausalELM.binary_step","page":"API","title":"CausalELM.binary_step","text":"binary_step(x)\n\nApply the binary step activation function.\n\nExamples\n\njulia> binary_step(1)\n1\n\njulia> binary_step([-1000, 100, 1, 0, -0.001, -3])\n6-element Vector{Int64}:\n 0\n 1\n 1\n 1\n 0\n 0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.σ","page":"API","title":"CausalELM.σ","text":"σ(x)\n\nApply the sigmoid activation function.\n\nExamples\n\njulia> σ(1)\n0.7310585786300049\n\njulia> σ([1.0, 0.0])\n2-element Vector{Float64}:\n 0.7310585786300049\n 0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.relu","page":"API","title":"CausalELM.relu","text":"relu(x)\n\nApply the ReLU activation function.\n\nExamples\n\njulia> relu(1)\n1\n\njulia> relu([1.0, 0.0, -1.0])\n3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.leaky_relu","page":"API","title":"CausalELM.leaky_relu","text":"leaky_relu(x)\n\nApply the leaky ReLU activation function to a number.\n\nExamples\n\njulia> leaky_relu(1)\n1\n\njulia> leaky_relu([-1.0, 0.0, 1.0])\n3-element Vector{Float64}:\n -0.01\n  0.0\n  1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.swish","page":"API","title":"CausalELM.swish","text":"swish(x)\n\nApply the swish activation function to a number.\n\nExamples\n\njulia> swish(1)\n0.7310585786300049\n\njulia> swish([1.0, -1.0])\n2-element Vector{Float64}:\n  0.7310585786300049\n -0.2689414213699951\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.softmax","page":"API","title":"CausalELM.softmax","text":"softmax(x)\n\nApply the softmax activation function to a number.\n\nExamples\n\njulia> softmax(1)\n1.0\n\njulia> softmax([1.0, 2.0, 3.0])\n3-element Vector{Float64}:\n 0.09003057317038045\n 0.24472847105479764\n 0.6652409557748219\n\njulia> softmax([1.0 2.0 3.0; 4.0 5.0 6.0])\n2×3 Matrix{Float64}:\n 0.0900306  0.244728  0.665241\n 0.0900306  0.244728  0.665241\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.softplus","page":"API","title":"CausalELM.softplus","text":"softplus(x)\n\nApply the softplus activation function to a number.\n\nExamples\n\njulia> softplus(1)\n1.3132616875182228\n\njulia> softplus([1.0, -1.0])\n2-element Vector{Float64}:\n 1.3132616875182228\n 0.3132616875182228\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gelu","page":"API","title":"CausalELM.gelu","text":"gelu(x)\n\nApply the GeLU activation function to a number.\n\nExamples\n\njulia> gelu(1)\n0.8411919906082768\n\njulia> gelu([-1.0, 0.0])\n2-element Vector{Float64}:\n -0.15880800939172324\n  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gaussian","page":"API","title":"CausalELM.gaussian","text":"gaussian(x)\n\nApply the gaussian activation function to a real number.\n\nExamples\n\njulia> gaussian(1)\n0.36787944117144233\n\njulia> gaussian([1.0, -1.0])\n2-element Vector{Float64}:\n 0.3678794411714423\n 0.3678794411714423\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.hard_tanh","page":"API","title":"CausalELM.hard_tanh","text":"hard_tanh(x)\n\nApply the hard_tanh activation function to a number.\n\nExamples\n\njulia> hard_tanh(-2)\n-1\n\njulia> hard_tanh([-2.0, 0.0, 2.0])\n3-element Vector{Real}:\n -1\n  0.0\n  1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.elish","page":"API","title":"CausalELM.elish","text":"elish(x)\n\nApply the ELiSH activation function to a number.\n\nExamples\n\njulia> elish(1)\n0.7310585786300049\n\njulia> elish([-1.0, 1.0])\n2-element Vector{Float64}:\n -0.17000340156854793\n  0.7310585786300049\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.fourier","page":"API","title":"CausalELM.fourier","text":"fourrier(x)\n\nApply the Fourier activation function to a real number.\n\nExamples\n\njulia> fourier(1)\n0.8414709848078965\n\njulia> fourier([-1.0, 1.0])\n2-element Vector{Float64}:\n -0.8414709848078965\n  0.8414709848078965\n\n\n\n\n\n","category":"function"},{"location":"api/#Cross-Validation","page":"API","title":"Cross Validation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.generate_folds\nCausalELM.generate_temporal_folds\nCausalELM.validation_loss\nCausalELM.cross_validate\nCausalELM.best_size\nCausalELM.shuffle_data","category":"page"},{"location":"api/#CausalELM.generate_folds","page":"API","title":"CausalELM.generate_folds","text":"generate_folds(X, Y, folds)\n\nCreate folds for cross validation.\n\nExamples\n\njulia> xfolds, y_folds = CausalELM.generate_folds(zeros(4, 2), zeros(4), 2)\n([[0.0 0.0], [0.0 0.0; 0.0 0.0; 0.0 0.0]], [[0.0], [0.0, 0.0, 0.0]])\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.generate_temporal_folds","page":"API","title":"CausalELM.generate_temporal_folds","text":"generate_temporal_folds(X, Y, folds)\n\nCreate rolling folds for cross validation of time series data.\n\nExamples\n\njulia> xfolds, yfolds = CausalELM.generate_temporal_folds([1 1; 1 1; 0 0; 0 0], zeros(4), 2)\n([[1 1; 1 1], [1 1; 1 1; 0 0; 0 0]], [[0.0, 0.0], [0.0, 0.0, 0.0, 0.0]])\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.validation_loss","page":"API","title":"CausalELM.validation_loss","text":"validation_loss(xtrain, ytrain, xtest, ytest, nodes, metric; kwargs...)\n\nCalculate a validation metric for a single fold in k-fold cross validation.\n\nArguments\n\nxtrain::Any: an array of features to train on.\nytrain::Any: an array of training labels.\nxtest::Any: an array of features to test on.\nytrain::Any: an array of testing labels.\nnodes::Int: the number of neurons in the extreme learning machine.\nmetric::Function: the validation metric to calculate.\n\nKeywords\n\nactivation::Function=relu: the activation function to use.\nregularized::Function=true: whether to use L2 regularization.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> validation_loss(x, y, 5, accuracy, 3)\n0.5402532843396273\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.cross_validate","page":"API","title":"CausalELM.cross_validate","text":"cross_validate(X, Y, neurons, metric, activation, regularized, folds, temporal)\n\nCalculate a validation metric for k folds using a single set of hyperparameters.\n\nArguments\n\nX::Array: array of features to train on.\nY::Vector: vector of labels to train on.\nneurons::Int: number of neurons to use in the extreme learning machine.\nmetric::Function: validation metric to calculate.\nactivation::Function=relu: activation function to use.\nregularized::Function=true: whether to use L2 regularization\nfolds::Int: number of folds to use for cross validation.\ntemporal::Function=true: whether the data is of a time series or panel nature.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> cross_validate(x, y, 5, accuracy)\n0.8891028047100136\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.best_size","page":"API","title":"CausalELM.best_size","text":"best_size(m)\n\nCompute the best number of neurons for an estimator.\n\nNotes\n\nThe procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [minneurons, maxneurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between minneurons and maxneurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.\n\nArguments\n\nm::Any: estimator to find the best number of neurons for.\n\nExamples\n\njulia> X, T, Y = rand(100, 5), rand(0:1, 100), rand(100)\njulia> m1 = GComputation(X, T, y)\njulia> best_size(m1)\n8\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.shuffle_data","page":"API","title":"CausalELM.shuffle_data","text":"shuffle_data(X, Y)\n\nShuffles covariates and outcome vector for cross validation.\n\nExamples\n\njulia> shuffle_data([1 1; 2 2; 3 3; 4 4], collect(1:4))\n([4 4; 2 2; 1 1; 3 3], [4, 2, 1, 3])\n\n\n\n\n\n","category":"function"},{"location":"api/#Average-Causal-Effect-Estimators","page":"API","title":"Average Causal Effect Estimators","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.estimate_effect!\ng_formula!\nCausalELM.causal_loss!\nCausalELM.predict_residuals\nCausalELM.make_folds\nCausalELM.moving_average","category":"page"},{"location":"api/#CausalELM.causal_loss!","page":"API","title":"CausalELM.causal_loss!","text":"causal_loss!(DML, [,cate])\n\nMinimize the causal loss function for double machine learning.\n\nNotes\n\nThis method should not be called directly.\n\nArguments\n\nDML::DoubleMachineLearning: the DoubleMachineLearning struct to estimate the effect for.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> causal_loss!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict_residuals","page":"API","title":"CausalELM.predict_residuals","text":"predict_residuals(D, x_train, x_test, y_train, y_test, t_train, t_test)\n\nPredict treatment and outcome residuals for double machine learning or R-learning.\n\nNotes\n\nThis method should not be called directly.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> x_train, x_test = X[1:80, :], X[81:end, :]\njulia> y_train, y_test = Y[1:80], Y[81:end]\njulia> t_train, t_test = T[1:80], T[81:100]\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> predict_residuals(m1, x_train, x_test, y_train, y_test, t_train, t_test)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.make_folds","page":"API","title":"CausalELM.make_folds","text":"make_folds(D)\n\nMake folds for cross fitting for a double machine learning estimator.\n\nNotes\n\nThis method should not be called directly.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> make_folds(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.moving_average","page":"API","title":"CausalELM.moving_average","text":"moving_average(x)\n\nCalculates a cumulative moving average.\n\nExamples\n\njulia> moving_average([1, 2, 3])\n\n\n\n\n\n","category":"function"},{"location":"api/#Metalearners","page":"API","title":"Metalearners","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.causal_loss\nCausalELM.doubly_robust_formula!\nCausalELM.stage1!\nCausalELM.stage2!","category":"page"},{"location":"api/#CausalELM.doubly_robust_formula!","page":"API","title":"CausalELM.doubly_robust_formula!","text":"doubly_robust_formula!(DRE, X, T, Y, Z)\n\nEstimate the CATE for a single cross fitting iteration via doubly robust estimation.\n\nThis method should not be called directly.\n\nArguments\n\nDRE::DoublyRobustLearner: the DoubleMachineLearning struct to estimate the effect for.\nX: a vector of three covariate folds.\nT: a vector of three treatment folds.\nY: a vector of three outcome folds.\nZ : a vector of three confounder folds and covariate folds.\n\nExamples\n\njulia> X, T, Y, W =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100), rand(6, 100)\njulia> m1 = DoublyRobustLearner(X, T, Y, W=W)\n\njulia> X, T, W, Y = make_folds(m1)\njulia> Z = m1.W == m1.X ? X : [reduce(hcat, (z)) for z in zip(X, W)]\njulia> g_formula!(m1, X, T, Y, Z)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.stage1!","page":"API","title":"CausalELM.stage1!","text":"stage1!(x)\n\nEstimate the first stage models for an X-learner.\n\nThis method should not be called by the user.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> stage1!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.stage2!","page":"API","title":"CausalELM.stage2!","text":"stage2!(x)\n\nEstimate the second stage models for an X-learner.\n\nThis method should not be called by the user.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> stage1!(m1)\njulia> stage2!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Common-Methods","page":"API","title":"Common Methods","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"estimate_causal_effect!","category":"page"},{"location":"api/#CausalELM.estimate_causal_effect!","page":"API","title":"CausalELM.estimate_causal_effect!","text":"estimate_causal_effect!(its)\n\nEstimate the effect of an event relative to a predicted counterfactual.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(DML)\n\nEstimate a causal effect of interest using double machine learning.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\njulia> W = rand(100, 6)\njulia> m2 = DoubleMachineLearning(X, T, Y, W=W)\njulia> estimate_causal_effect!(m2)\n\n\n\n\n\nestimate_causal_effect!(s)\n\nEstimate the CATE using an S-learner.\n\nFor an overview of S-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m4 = SLearner(X, T, Y)\njulia> estimate_causal_effect!(m4)\n\n\n\n\n\nestimate_causal_effect!(t)\n\nEstimate the CATE using an T-learner.\n\nFor an overview of T-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m5 = TLearner(X, T, Y)\njulia> estimate_causal_effect!(m5)\n\n\n\n\n\nestimate_causal_effect!(x)\n\nEstimate the CATE using an X-learner.\n\nFor an overview of X-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(R)\n\nEstimate the CATE using an R-learner.\n\nFor an overview of R-learning see:     Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment      effects.\" Biometrika 108, no. 2 (2021): 299-319.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = RLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\nestimate_causal_effect!(DRE)\n\nEstimate the CATE using a doubly robust learner.\n\nFor details on how this method estimates the CATE see:     Kennedy, Edward H. \"Towards optimal doubly robust estimation of heterogeneous causal      effects.\" Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoublyRobustLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Inference","page":"API","title":"Inference","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"summarize\nCausalELM.generate_null_distribution\nCausalELM.quantities_of_interest","category":"page"},{"location":"api/#CausalELM.summarize","page":"API","title":"CausalELM.summarize","text":"summarize(mod, n)\n\nGet a summary from a CausalEstimator or Metalearner.\n\nArguments\n\nmod::Union{CausalEstimator, Metalearner}: a model to summarize.\nn::Int=100: the number of iterations to generate the numll distribution for    randomization inference.\n\nNotes\n\np-values and standard errors are estimated using approximate randomization inference.\n\nReferences\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> X, T, Y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = GComputation(X, T, Y)\njulia> estimate_causal_effect!(m1)\njulia> summarize(m1)\n\njulia> m2 = RLearner(X, T, Y)\njulia> estimate_causal_effect(m2)\njulia> summarize(m2)\n\njulia> m3 = SLearner(X, T, Y)\njulia> estimate_causal_effect!(m3)\njulia> summarise(m3)  # British spelling works too!\n\n\n\n\n\nsummarize(its, n, mean_effect)\n\nGet a summary from an interrupted time series estimator.\n\nArguments\n\nits::InterruptedTimeSeries: interrupted time series estimator\nn::Int=100: number of iterations to generate the numll distribution for randomization    inference.\nmean_effect::Bool=true: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m4 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m4)\njulia> summarize(m4)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.generate_null_distribution","page":"API","title":"CausalELM.generate_null_distribution","text":"generate_null_distribution(mod, n)\n\nGenerate a null distribution for the treatment effect of G-computation, double machine  learning, or metalearning.\n\nArguments\n\nmod::Any: model to summarize.\nn::Int=100: number of iterations to generate the null distribution for randomization    inference.\n\nNotes\n\nThis method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When mod is a metalearner the null statistic is the difference is the ATE.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nExamples\n\njulia> x, t, y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(1:100, 100, 1)\njulia> g_computer = GComputation(x, t, y)\njulia> estimate_causal_effect!(g_computer)\njulia> generate_null_distribution(g_computer, 500)\n\n\n\n\n\ngenerate_null_distribution(its, n, mean_effect)\n\nArguments\n\nits::InterruptedTimeSeries: interrupted time series estimator\nn::Int=100: number of iterations to generate the numll distribution for randomization    inference.\nmean_effect::Bool=true: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> generate_null_distribution(its, 10)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.quantities_of_interest","page":"API","title":"CausalELM.quantities_of_interest","text":"quantities_of_interest(mod, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x, t, y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(1:100, 100, 1)\njulia> g_computer = GComputation(x, t, y)\njulia> estimate_causal_effect!(g_computer)\njulia> quantities_of_interest(g_computer, 1000)\n\n\n\n\n\nquantities_of_interest(mod, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> quantities_of_interest(its, 10)\n\n\n\n\n\n","category":"function"},{"location":"api/#Model-Validation","page":"API","title":"Model Validation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"validate\nCausalELM.covariate_independence\nCausalELM.omitted_predictor\nCausalELM.sup_wald\nCausalELM.p_val\nCausalELM.counterfactual_consistency\nCausalELM.simulate_counterfactual_violations\nCausalELM.exchangeability\nCausalELM.e_value\nCausalELM.binarize\nCausalELM.risk_ratio\nCausalELM.positivity\nCausalELM.var_type","category":"page"},{"location":"api/#CausalELM.validate","page":"API","title":"CausalELM.validate","text":"validate(its; kwargs...)\n\nTest the validity of an estimated interrupted time series analysis.\n\nArguments\n\nits::InterruptedTimeSeries: an interrupted time seiries estimator.\n\nKeywords\n\nn::Int: number of times to simulate a confounder.\nlow::Float64=0.15: minimum proportion of data points to include before or after the    tested break in the Wald supremum test.\nhigh::Float64=0.85: maximum proportion of data points to include before or after the    tested break in the Wald supremum test.\n\nNotes\n\nThis method coducts a Chow Test, a Wald supremeum test, and tests the model's sensitivity to  confounders. The Chow Test tests for structural breaks in the covariates between the time  before and after the event. p-values represent the proportion of times the magnitude of the  break in a covariate would have been greater due to chance. Lower p-values suggest a higher  probability the event effected the covariates and they cannot provide unbiased  counterfactual predictions. The Wald supremum test finds the structural break with the  highest Wald statistic. If this is not the same as the hypothesized break, it could indicate  an anticipation effect, a confounding event, or that the intervention or policy took place  in multiple phases. p-values represent the proportion of times we would see a larger Wald  statistic if the data points were randomly allocated to pre and post-event periods for the  predicted structural break. Ideally, the hypothesized break will be the same as the  predicted break and it will also have a low p-value. The omitted predictors test adds  normal random variables with uniform noise as predictors. If the included covariates are  good predictors of the counterfactual outcome, adding irrelevant predictors should not have  a large effect on the predicted counterfactual outcomes or the estimated effect.\n\nThis method does not implement the second test in Baicker and Svoronos because the estimator  in this package models the relationship between covariates and the outcome and uses an  extreme learning machine instead of linear regression, so variance in the outcome across  different bins is not much of an issue.\n\nReferences\n\nFor more details on the assumptions and validity of interrupted time series designs, see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ = rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\njulia> validate(m1)\n\n\n\n\n\nvalidate(m; kwargs)\n\nArguments\n\nm::Union{CausalEstimator, Metalearner}: model to validate/test the assumptions of.\n\nKeywords\n\ndevs=::Any: iterable of deviations from which to generate noise to simulate violations    of the counterfactual consistency assumption.\nnum_iterations=10::Int: number of times to simulate a violation of the counterfactual    consistency assumption.\nmin::Float64=1.0e-6: minimum probability of treatment for the positivity assumption.\nhigh::Float64=1-min: maximum probability of treatment for the positivity assumption.\n\nNotes\n\nThis method tests the counterfactual consistency, exchangeability, and positivity  assumptions required for causal inference. It should be noted that consistency and  exchangeability are not directly testable, so instead, these tests do not provide definitive  evidence of a violation of these assumptions. To probe the counterfactual consistency  assumption, we simulate counterfactual outcomes that are different from the observed  outcomes, estimate models with the simulated counterfactual outcomes, and take the averages. If the outcome is continuous, the noise for the simulated counterfactuals is drawn from  N(0, dev) for each element in devs, otherwise the default is 0.25, 0.5, 0.75, and 1.0  standard deviations from the mean outcome. For discrete variables, each outcome is replaced  with a different value in the range of outcomes with probability ϵ for each ϵ in devs,  otherwise the default is 0.025, 0.05, 0.075, 0.1. If the average estimate for a given level  of violation differs greatly from the effect estimated on the actual data, then the model is  very sensitive to violations of the counterfactual consistency assumption for that level of  violation. Next, this methods tests the model's sensitivity to a violation of the  exchangeability assumption by calculating the E-value, which is the minimum strength of  association, on the risk ratio scale, that an unobserved confounder would need to have with  the treatment and outcome variable to fully explain away the estimated effect. Thus, higher  E-values imply the model is more robust to a violation of the exchangeability assumption.  Finally, this method tests the positivity assumption by estimating propensity scores. Rows  in the matrix are levels of covariates that have a zero probability of treatment. If the  matrix is empty, none of the observations have an estimated zero probability of treatment,  which implies the positivity assumption is satisfied.\n\nReferences\n\nFor a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. \n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]), vec(rand(1:100, 100, 1)) \njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> validate(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.covariate_independence","page":"API","title":"CausalELM.covariate_independence","text":"covariate_independence(its; kwargs..)\n\nTest for independence between covariates and the event or intervention.\n\nArguments\n\nits::InterruptedTImeSeries: an interrupted time series estimator.\n\nKeywords\n\nn::Int: number of permutations for assigning observations to the pre and        post-treatment periods.\n\nThis is a Chow Test for covariates with p-values estimated via randomization inference,  which does not assume a distribution for the outcome variable. The p-values are the  proportion of times randomly assigning observations to the pre or post-intervention period  would have a larger estimated effect on the the slope of the covariates. The lower the  p-values, the more likely it is that the event/intervention effected the covariates and  they cannot provide an unbiased prediction of the counterfactual outcomes.\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n       randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> covariate_independence(its)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.omitted_predictor","page":"API","title":"CausalELM.omitted_predictor","text":"omitted_predictor(its; kwargs...)\n\nSee how an omitted predictor/variable could change the results of an interrupted time series  analysis.\n\nArguments\n\nits::InterruptedTImeSeries: interrupted time seiries estimator.\n\nKeywords\n\nn::Int: number of times to simulate a confounder.\n\nNotes\n\nThis method reestimates interrupted time series models with uniform random variables. If the  included covariates are good predictors of the counterfactual outcome, adding a random  variable as a covariate should not have a large effect on the predicted counterfactual  outcomes and therefore the estimated average effect.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> omitted_predictor(its)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.sup_wald","page":"API","title":"CausalELM.sup_wald","text":"sup_wald(its; kwargs)\n\nCheck if the predicted structural break is the hypothesized structural break.\n\nArguments\n\nits::InterruptedTimeSeries: interrupted time seiries estimator.\n\nKeywords\n\nn::Int: number of times to simulate a confounder.\nlow::Float64=0.15: minimum proportion of data points to include before or after the        tested break in the Wald supremum test.\nhigh::Float64=0.85: maximum proportion of data points to include before or after the        tested break in the Wald supremum test.\n\nNotes\n\nThis method conducts Wald tests and identifies the structural break with the highest Wald  statistic. If this break is not the same as the hypothesized break, it could indicate an  anticipation effect, confounding by some other event or intervention, or that the  intervention or policy took place in multiple phases. p-values are estimated using  approximate randomization inference and represent the proportion of times we would see a  larger Wald statistic if the data points were randomly allocated to pre and post-event  periods for the predicted structural break.\n\nReferences\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n       randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> sup_wald(its)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.p_val","page":"API","title":"CausalELM.p_val","text":"p_val(x, y, β; kwargs...)\n\nEstimate the p-value for the hypothesis that an event had a statistically significant effect  on the slope of a covariate using randomization inference.\n\nArguments\n\nx::Array{<:Real}: covariates.\ny::Array{<:Real}: outcome.\nβ::Array{<:Real}=0.15: fitted weights.\n\nKeywords\n\ntwo_sided::Bool=false: whether to conduct a one-sided hypothesis test.\n\nExamples\n\njulia> x, y, β = reduce(hcat, (float(rand(0:1, 10)), ones(10))), rand(10), 0.5\njulia> p_val(x, y, β)\njulia> p_val(x, y, β; n=100, two_sided=true)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.counterfactual_consistency","page":"API","title":"CausalELM.counterfactual_consistency","text":"counterfactual_consistency(m; kwargs...)\n\nArguments\n\nm::Union{CausalEstimator, Metalearner}: model to validate/test the assumptions of.\n\nKeywords\n\nnum_devs=(0.25, 0.5, 0.75, 1.0)::Tuple: number of standard deviations from which to    generate noise from a normal distribution to simulate violations of the counterfactual    consistency assumption.\nnum_iterations=10::Int: number of times to simulate a violation of the counterfactual    consistency assumption.\n\nNotes\n\nExamine the counterfactual consistency assumption. First, this function simulates  counterfactual outcomes that are offset from the outcomes in the dataset by random scalars drawn from a N(0, numstddev). Then, the procedure is repeated numiterations times and  averaged. If the model is a metalearner, then the estimated individual treatment effects  are averaged and the mean CATE is averaged over all the iterations, otherwise the estimated  treatment effect is averaged over the iterations. The previous steps are repeated for each  element in numdevs.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> counterfactual_consistency(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.simulate_counterfactual_violations","page":"API","title":"CausalELM.simulate_counterfactual_violations","text":"simulate_counterfactual_violations(y, dev)\n\nArguments\n\ny::Vector{<:Real}: vector of real-valued outcomes.\ndev::Float64: deviation of the observed outcomes from the true counterfactual outcomes.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]), vec(rand(1:100, 100, 1)) \njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> simulate_counterfactual_violations(g_computer)\n-0.7748591231872396\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.exchangeability","page":"API","title":"CausalELM.exchangeability","text":"exchangeability(model)\n\nTest the sensitivity of a G-computation or doubly robust estimator or metalearner to a  violation of the exchangeability assumption.\n\nReferences\n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> e_value(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.e_value","page":"API","title":"CausalELM.e_value","text":"e_value(model)\n\nTest the sensitivity of an estimator to a violation of the exchangeability assumption.\n\nReferences\n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> e_value(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.binarize","page":"API","title":"CausalELM.binarize","text":"binarize(x, cutoff)\n\nConvert a vector of counts or a continuous vector to a binary vector.\n\nArguments\n\nx::Any: interable of numbers to binarize.\nx::Any: threshold after which numbers are converted to 1 and befrore which are converted    to 0.\n\nExamples\n\njulia> CausalELM.binarize([1, 2, 3], 2)\n3-element Vector{Int64}:\n 0\n 0\n 1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.risk_ratio","page":"API","title":"CausalELM.risk_ratio","text":"risk_ratio(model)\n\nCalculate the risk ratio for an estimated model.\n\nNotes\n\nIf the treatment variable is not binary and the outcome variable is not continuous then the  treatment variable will be binarized.\n\nReferences\n\nFor more information on how other quantities of interest are converted to risk ratios see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> risk_ratio(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.positivity","page":"API","title":"CausalELM.positivity","text":"positivity(model, [,min], [,max])\n\nFind likely violations of the positivity assumption.\n\nNotes\n\nThis method uses an extreme learning machine or regularized extreme learning machine to  estimate probabilities of treatment. The returned matrix, which may be empty, are the  covariates that have a (near) zero probability of treatment or near zero probability of  being assigned to the control group, whith their entry in the last column being their  estimated treatment probability. In other words, they likely violate the positivity  assumption.\n\nArguments\n\nmodel::Union{CausalEstimator, Metalearner}: a model to validate/test the assumptions of.\nmin::Float64=1.0e-6: minimum probability of treatment for the positivity assumption.\nhigh::Float64=1-min: the maximum probability of treatment for the positivity assumption.\n\nExamples\n\njulia> x, t = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]\njulia> y = vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> positivity(g_computer)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.var_type","page":"API","title":"CausalELM.var_type","text":"var_type(x)\n\nDetermine the type of variable held by a vector.\n\nExamples\n\njulia> CausalELM.var_type([1, 2, 3, 2, 3, 1, 1, 3, 2])\nCausalELM.Count()\n\n\n\n\n\n","category":"function"},{"location":"api/#Validation-Metrics","page":"API","title":"Validation Metrics","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"mse\nmae\naccuracy\nprecision\nrecall\nF1\nCausalELM.confusion_matrix","category":"page"},{"location":"api/#CausalELM.mse","page":"API","title":"CausalELM.mse","text":"mse(y, ŷ)\n\nCalculate the mean squared error\n\nSee also mae.\n\nExamples\n\njulia> mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n4.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.mae","page":"API","title":"CausalELM.mae","text":"mae(y, ŷ)\n\nCalculate the mean absolute error\n\nSee also mse.\n\nExamples\n\njulia> mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n2.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.accuracy","page":"API","title":"CausalELM.accuracy","text":"accuracy(y, ŷ)\n\nCalculate the accuracy for a classification task\n\nExamples\n\njulia> accuracy([1, 1, 1, 1], [0, 1, 1, 0])\n0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.recall","page":"API","title":"CausalELM.recall","text":"recall(y, ŷ)\n\nCalculate the recall for a classification task\n\nSee also precision.\n\nExamples\n\njulia> recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.F1","page":"API","title":"CausalELM.F1","text":"F1(y, ŷ)\n\nCalculate the F1 score for a classification task\n\nExamples\n\njulia> F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.confusion_matrix","page":"API","title":"CausalELM.confusion_matrix","text":"confusion_matrix(y, ŷ)\n\nGenerate a confusion matrix\n\nExamples\n\njulia> CausalELM.confusion_matrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\n\n\n\n\n","category":"function"},{"location":"api/#Extreme-Learning-Machines","page":"API","title":"Extreme Learning Machines","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.fit!\nCausalELM.predict\nCausalELM.predict_counterfactual!\nCausalELM.placebo_test\nCausalELM.ridge_constant\nCausalELM.set_weights_biases","category":"page"},{"location":"api/#CausalELM.fit!","page":"API","title":"CausalELM.fit!","text":"fit!(model)\n\nMake predictions with an ExtremeLearner.\n\nReferences\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\n\n\n\n\n\nfit!(model)\n\nFit a Regularized Extreme Learner.\n\nReferences\n\nFor more details see:      Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\njulia> f1 = fit!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict","page":"API","title":"CausalELM.predict","text":"predict(model, X)\n\nUse an ExtremeLearningMachine to make predictions.\n\nReferences\n\nFor more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\njulia> f1 = fit(m1, sigmoid)\njulia> predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict_counterfactual!","page":"API","title":"CausalELM.predict_counterfactual!","text":"predict_counterfactual!(model, X)\n\nUse an ExtremeLearningMachine to predict the counterfactual.\n\nNotes\n\nThis should be run with the observed covariates. To use synthtic data for what-if scenarios  use predict.\n\nSee also predict.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\njulia> f1 = fit(m1, sigmoid)\njulia> predict_counterfactual!(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.placebo_test","page":"API","title":"CausalELM.placebo_test","text":"placebo_test(model)\n\nConduct a placebo test.\n\nNotes\n\nThis method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.\n\nExamples\n\njulia> x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]\njulia> m1 = ExtremeLearner(x, y, 10, σ)\njulia> f1 = fit(m1, sigmoid)\njulia> predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\njulia> placebo_test(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ridge_constant","page":"API","title":"CausalELM.ridge_constant","text":"ridge_constant(model, [,iterations])\n\nCalculate the L2 penalty for a regularized extreme learning machine using generalized cross  validation with successive halving.\n\nArguments\n\nmodel::RegularizedExtremeLearner: regularized extreme learning machine.\niterations::Int: number of iterations to perform for successive halving.\n\nReferences\n\nFor more information see:      Golub, Gene H., Michael Heath, and Grace Wahba. \"Generalized cross-validation as a      method for choosing a good ridge parameter.\" Technometrics 21, no. 2 (1979): 215-223.\n\nExamples\n\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\njulia> ridge_constant(m1)\njulia> ridge_constant(m1, iterations=20)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.set_weights_biases","page":"API","title":"CausalELM.set_weights_biases","text":"set_weights_biases(model)\n\nCalculate the weights and biases for an extreme learning machine or regularized extreme  learning machine.\n\nNotes\n\nInitialization is done using uniform Xavier initialization.\n\nReferences\n\nFor details see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\njulia> set_weights_biases(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Utility-Functions","page":"API","title":"Utility Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.mean\nCausalELM.var\nCausalELM.consecutive\nCausalELM.one_hot_encode\nCausalELM.clip_if_binary\nCausalELM.model_config\nCausalELM.standard_input_data\nCausalELM.double_learner_input_data","category":"page"},{"location":"api/#CausalELM.mean","page":"API","title":"CausalELM.mean","text":"mean(x)\n\nCalculate the mean of a vector.\n\nExamples\n\njulia> CausalELM.mean([1, 2, 3, 4])\n2.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.var","page":"API","title":"CausalELM.var","text":"var(x)\n\nCalculate the (sample) mean of a vector.\n\nExamples\n\njulia> CausalELM.var([1, 2, 3, 4])\n1.6666666666666667\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.one_hot_encode","page":"API","title":"CausalELM.one_hot_encode","text":"one_hot_encode(x)\n\nOne hot encode a categorical vector for multiclass classification.\n\nExamples\n\njulia> CausalELM.one_hot_encode([1, 2, 3, 4, 5])\n5×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.clip_if_binary","page":"API","title":"CausalELM.clip_if_binary","text":"clip_if_binary(x, var)\n\nConstrain binary values between 1e-7 and 1 - 1e-7, otherwise return the original values.\n\nArguments\n\nx::Array: array to clip if it is binary.\nvar: type of x based on calling var_type.\n\nSee also var_type.\n\nExamples\n\njulia> CausalELM.clip_if_binary([1.2, -0.02], CausalELM.Binary())\n2-element Vector{Float64}:\n 0.9999999\n 1.0e-7\n\njulia> CausalELM.clip_if_binary([1.2, -0.02], CausalELM.Count())\n2-element Vector{Float64}:\n  1.2\n -0.02\n\n\n\n\n\n","category":"function"},{"location":"guide/estimatorselection/#Deciding-Which-Estimator-to-Use","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"","category":"section"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"Which model you should use depends on what you are trying to model and the type of data you  have. The table below can serve as a useful reference when deciding which model to use for a  given dataset and causal question.","category":"page"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"Model Struct Causal Estimands Supported Treatment Types Supported Outcome Types\nInterrupted Time Series Analysis InterruptedTimeSeries ATE, Cumulative Treatment Effect Binary Continuous, Count[2], Time to Event\nG-computation GComputation ATE, ATT, ITT Binary Binary[1],Continuous, Time to Event, Count[2]\nDouble Machine Learning DoubleMachineLearning ATE Binary[1], Count[2], Continuous Binary[1], Count[2], Continuous, Time to Event\nS-learning SLearner CATE Binary Binary[1], Continuous, Time to Event, Count[2]\nT-learning TLearner CATE Binary Binary[1], Continuous, Count[2], Time to Event\nX-learning XLearner CATE Binary[1] Binary[1], Continuous, Count[2], Time to Event\nR-learning RLearner CATE Binary[1], Count[2], Continuous Binary[1], Count[2], Continuous, Time to Event\nDoubly Robust Estimation DoublyRobustLearner CATE Binary Binary[1], Continuous, Count[2], Time to Event","category":"page"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"[1]: Models that use propensity scores or predict binary treatment assignment may, on very rare occasions, return values outside of [0, 1]. In that case, values are clipped to be between 0.0000001 and 0.9999999.","category":"page"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"[2]: Similar to other packages, predictions of count variables is treated as a continuous regression task.","category":"page"},{"location":"guide/gcomputation/#G-Computation","page":"G-computation","title":"G-Computation","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"In some cases, we may want to know the causal effect of a treatment that varies and is  confounded over time. For example, a doctor might want to know the effect of a treatment  given at multiple times whose status depends on the health of the patient at a given time.  One way to get an unbiased estimate of the causal effect is to use G-computation. The basic  steps for using G-computation in CausalELM are below.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nFor a good overview of G-Computation see:Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé \nRousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann \nFoucher. \"G-computation, propensity score-based methods, and targeted maximum likelihood \nestimator for causal inference with different covariates sets: a comparative simulation \nstudy.\" Scientific reports 10, no. 1 (2020): 9219.","category":"page"},{"location":"guide/gcomputation/#Step-1:-Initialize-a-Model","page":"G-computation","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"The GComputation method takes at least three arguments: an array of covariates, a vector of  treatment statuses, and an outcome vector. It can support binary treatments and binary,  continuous, time to event, and count outcome variables.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"tip: Tip\nYou can also specify the causal estimand, whether to employ L2 regularization, which  activation function to use, whether the data is of a temporal nature, the metric to use when  using cross validation to find the best number of neurons, the minimum number of neurons to  consider, the maximum number of neurons to consider, the number of folds to use during cross  caidation, and the number of neurons to use in the ELM that learns a mapping from number of  neurons to validation loss. These options are specified with the following keyword  arguments: quantity_of_interest, regularized, activation, temporal, validation_metric,  min_neurons, max_neurons, folds, iterations, and approximator_neurons.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nInternally, the outcome model is treated as a regression since extreme learning machines  minimize the MSE. This means that predicted outcomes under treatment and control groups  could fall outside [0, 1], although this is not likely in practice. To deal with this,  predicted binary variables are automatically clipped to [0.0000001, 0.9999999]. This also  means that count outcomes will be predicted as continuous variables.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Create some data with a binary treatment\nX, T, Y =  rand(1000, 5), [rand()<0.4 for i in 1:1000], rand(1000)\n\n# We could also use DataFrames\n# using DataFrames\n# X = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:1000]), DataFrame(y=rand(1000))\n\ng_computer = GComputation(X, T, Y)","category":"page"},{"location":"guide/gcomputation/#Step-2:-Estimate-the-Causal-Effect","page":"G-computation","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"To estimate the causal effect, we pass the model above to estimatecausaleffect!.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Note that we could also estimate the ATT by setting quantity_of_interest=\"ATT\"\nestimate_causal_effect!(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-3:-Get-a-Summary","page":"G-computation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We get a summary of the model that includes a p-value and standard error estimated via  asymptotic randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"Calling the summarize method returns a dictionary with the estimator's task (regression or  classification), the quantity of interest being estimated (ATE or ATT), whether the model  uses an L2 penalty, the activation function used in the model's outcome predictors, whether  the data is temporal, the validation metric used for cross validation to find the best  number of neurons, the number of neurons used in the ELMs used by the estimator, the number  of neurons used in the ELM used to learn a mapping from number of neurons to validation  loss during cross validation, the causal effect, standard error, and p-value.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"summarize(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-4:-Validate-the-Model","page":"G-computation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we simulate counterfactual outcomes that are  different from the observed outcomes, estimate models with the simulated counterfactual  outcomes, and take the averages. If the outcome is continuous, the noise for the simulated  counterfactuals is drawn from N(0, dev) for each element in devs, otherwise the default is  0.25, 0.5, 0.75, and 1.0 standard deviations from the mean outcome. For discrete variables,  each outcome is replaced with a different value in the range of outcomes with probability ϵ  for each ϵ in devs, otherwise the default is 0.025, 0.05, 0.075, 0.1. If the average  estimate for a given level of violation differs greatly from the effect estimated on the  actual data, then the model is very sensitive to violations of the counterfactual  consistency assumption for that level of violation. Next, this method tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"tip: Tip\nOne can also specify the maxium number of possible treatments to consider for the causal  consistency assumption and the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for G-computation.  If the assumptions are not met then any estimates may be biased and lead to incorrect  conclusions.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"validate(g_computer)","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All contributions are welcome. To ensure contributions align with the existing code base and  are not duplicated, please follow the guidelines below.","category":"page"},{"location":"contributing/#Reporting-a-Bug","page":"Contributing","title":"Reporting a Bug","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To report a bug, open an issue on the CausalELM GitHub page. Please include all relevant information, such as what methods were called, the operating system used, the  verion/s of causalELM used, the verion/s of Julia used, any tracebacks or error codes, and  any other information that would be helpful for debugging. Also be sure to use the bug label.","category":"page"},{"location":"contributing/#Requesting-New-Features","page":"Contributing","title":"Requesting New Features","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before requesting a new feature, please check the issues page on GitHub to make sure someone else did not already request the same feature. If this is not the case, then please  open an issue that explains what function or method you would like to be added and how you  believe it should behave. Also be sure to use the enhancement tag.","category":"page"},{"location":"contributing/#Contributing-Code","page":"Contributing","title":"Contributing Code","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before submitting a pull request, please  open an issue explaining what the proposed code is and why you want to add it, if there is  not already an issue that addresses your changes and you are not fixing something very  minor. When submitting a pull request, please reference the relevant issue/s and ensure your  code follows the guidelines below.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before being merged, all pull requests should be well tested and all tests must be passing.\nAll abstract types, structs, functions, methods, macros, and constants have docstrings    that follow the same format as the other docstrings. These functions should also be    included in the relevant section of the API Manual.\nMost new structs for estimating causal effects should have mostly the same fields. To    reduce the burden of repeatedly defining all these fields, it is advisable to use the    modelconfig, standardinputdata, and doublelearnerinputdata macros to    programmatically generate fields for new structs. Doing so will ensure that with little    to no effort the new structs will work with the summarize and validate methods.\nThere are no repeated code blocks. If there are repeated codeblocks, then they should be    consolidated into a separate function.\nMethods should generally include types and be type stable. If there is a strong reason    to deviate from this point, there should be a comment in the code explaining why.\nMinimize use of new constants and macros. If they must be included, the reason for their    inclusion should be obvious or included in the docstring.\nAvoid using global variables and constants.\nCode should take advantage of Julia's built in macros for performance. Use @inbounds,    @view, @fastmath, and @simd when possible.\nWhen appending to an array in a loop, preallocate the array and update its values by    index.\nAvoid long functions and decompose them into smaller functions or methods. A general    rule is that function definitions should fit within the screen of a laptop.\nUse self-explanatory names for variables, methods, structs, constants, and macros.\nMake generous use of whitespace.\nAll functions should include docstrings.\n**  Docstrings may contain arguments, keywords, notes, references, and examples sections        in that order but some sections may be skipped.\n**  At a minimum, docstrings should contain the signature/s, a short description, and        examples\n**  Each section should include its own level one header.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nCausalELM follows the Blue style guide and all code is automatically formatted to  conform with this standard upon being pushed to GitHub.","category":"page"},{"location":"contributing/#Updating-or-Fixing-Documentation","page":"Contributing","title":"Updating or Fixing Documentation","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To propose a change to the documentation please submit an issue  or pull request.","category":"page"},{"location":"guide/doublemachinelearning/#Double-Machine-Learning","page":"Double Machine Learning","title":"Double Machine Learning","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"Double machine learning, also called debiased or orthogonalized machine learning, enables estimating causal effects when the dimensionality of the covariates is too high for linear  regression or the treatment or outcomes cannot be easily modeled parametrically. Double  machine learning estimates models of the treatment assignment and outcome and then combines  them in a final model. This is a semiparametric model in the sense that the first stage  models can take on any functional form but the final stage model is linear.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nFor more information see:Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,  Whitney Newey, and James Robins. \"Double/debiased machine learning for treatment and  structural parameters.\" (2018): C1-C68.","category":"page"},{"location":"guide/doublemachinelearning/#Step-1:-Initialize-a-Model","page":"Double Machine Learning","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"The DoubleMachineLearning constructor takes at least three arguments, an array of  covariates, a treatment vector, and an outcome vector. This estimator supports binary, count,  or continuous treatments and binary, count, continuous, or time to event outcomes. You can  also specify confounders that you do not want to estimate the CATE for by passing a parameter  to the W argument. Otherwise, the model assumes all possible confounders are contained in X.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nInternally, the outcome and treatment models are treated as a regression since extreme  learning machines minimize the MSE. This means that predicted treatments and outcomes  under treatment and control groups could fall outside [0, 1], although this is not likely  in practice. To deal with this, predicted binary variables are automatically clipped to  [0.0000001, 0.9999999]. This also means that count outcomes will be predicted as continuous  variables.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"tip: Tip\nYou can also specify the following options: whether the treatment vector is categorical ie  not continuous and containing more than two classes, whether to use L2 regularization, the  activation function, the validation metric to use when searching for the best number of  neurons, the minimum and maximum number of neurons to consider, the number of folds to use  for cross validation, the number of iterations to perform cross validation, and the number  of neurons to use in the ELM used to learn the function from number of neurons to validation  loss. These arguments are specified with the following keyword arguments: t_cat,  regularized, activation, validation_metric, min_neurons, max_neurons, folds, iterations,  and approximator_neurons.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Create some data with a binary treatment\nX, T, Y, W = rand(100, 5), [rand()<0.4 for i in 1:100], rand(100), rand(100, 4)\n\n# We could also use DataFrames\n# using DataFrames\n# X = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100), x5=rand(100))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:100]), DataFrame(y=rand(100))\n# W = DataFrame(w1=rand(100), w2=rand(100), w3=rand(100), w4=rand(100))\n\n# W is optional and means there are confounders that you are not interested in estimating\n# the CATE for\ndml = DoubleMachineLearning(X, T, Y, W=W)","category":"page"},{"location":"guide/doublemachinelearning/#Step-2:-Estimate-the-Causal-Effect","page":"Double Machine Learning","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"To estimate the causal effect, we call estimatecausaleffect! on the model above.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# we could also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimate_causal_effect!(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Get-a-Summary","page":"Double Machine Learning","title":"Get a Summary","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can get a summary that includes a p-value and standard error estimated via asymptotic  randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"Calling the summarize method returns a dictionary with the estimator's task (regression or  classification), the quantity of interest being estimated (ATE), whether the model uses an  L2 penalty (always true for DML), the activation function used in the model's outcome  predictors, whether the data is temporal (always false for DML), the validation metric used  for cross validation to find the best number of neurons, the number of neurons used in the  ELMs used by the estimator, the number of neurons used in the ELM used to learn a mapping  from number of neurons to validation loss during cross validation, the causal effect,  standard error, and p-value.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Can also use the British spelling\n# summarise(dml)\n\nsummarize(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Step-4:-Validate-the-Model","page":"Double Machine Learning","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we simulate counterfactual outcomes that are  different from the observed outcomes, estimate models with the simulated counterfactual  outcomes, and take the averages. If the outcome is continuous, the noise for the simulated  counterfactuals is drawn from N(0, dev) for each element in devs, otherwise the default is  0.25, 0.5, 0.75, and 1.0 standard deviations from the mean outcome. For discrete variables,  each outcome is replaced with a different value in the range of outcomes with probability ϵ  for each ϵ in devs, otherwise the default is 0.025, 0.05, 0.075, 0.1. If the average  estimate for a given level of violation differs greatly from the effect estimated on the  actual data, then the model is very sensitive to violations of the counterfactual  consistency assumption for that level of violation. Next, this method tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"tip: Tip\nOne can also specify the maxium number of possible treatments to consider for the causal  consistency assumption and the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for double machine  learning. If the assumptions are not met then any estimates may be biased and lead to  incorrect conclusions.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"validate(g_computer)","category":"page"},{"location":"guide/its/#Interrupted-Time-Series-Analysis","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Analysis","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Sometimes we want to know how an outcome variable for a single unit changed after an event  or intervention. For example, if regulators announce sanctions against company A, we might  want to know how the price of stock A changed after the announcement. Since we do not know what the price of Company A's stock would have been if the santions were not announced, we need some way to predict those values. An interrupted time series analysis does this by  using some covariates that are related to the oucome variable but not related to whether the  event happened to predict what would have happened. The estimated effects are the  differences between the predicted post-event counterfactual outcomes and the observed  post-event outcomes, which can also be aggregated to mean or cumulative effects.  Estimating an interrupted time series design in CausalELM consists of three steps.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nFor a deeper dive on interrupted time series estimation see:Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. \"Interrupted time series \nregression for the evaluation of public health interventions: a tutorial.\" International \njournal of epidemiology 46, no. 1 (2017): 348-355.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nThe flavor of interrupted time series implemented here is similar to the variant proposed  in:Brodersen, Kay H., Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L. Scott. \n\"Inferring causal impact using Bayesian structural time-series models.\" (2015): 247-274.in that, although it is not Bayesian, it uses a nonparametric model of the pre-treatment  period and uses that model to forecast the counterfactual in the post-treatment period, as  opposed to the commonly used segment linear regression.","category":"page"},{"location":"guide/its/#Step-1:-Initialize-an-interrupted-time-series-estimator","page":"Interrupted Time Series Estimation","title":"Step 1: Initialize an interrupted time series estimator","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"The InterruptedTimeSeries method takes at least four agruments: an array of pre-event  covariates, a vector of pre-event outcomes, an array of post-event covariates, and a vector  of post-event outcomes. The interrupted time series estimator assumes outcomes are either  continuous, count, or time to event variables.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nSince extreme learning machines minimize the MSE, count outcomes will be predicted as  continuous variables.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"tip: Tip\nYou can also specify whether or not to use L2 regularization, which activation function to  use, the metric to use when using cross validation to find the best number of neurons, the  minimum number of neurons to consider, the maximum number of neurons to consider, the number  of folds to use during cross caidation, the number of neurons to use in the ELM that learns  a mapping from number of neurons to validation loss, and whether to include a rolling  average autoregressive term. These options can be specified using the keyword arguments  regularized, activation, validation_metric, min_neurons, max_neurons, folds, iterations,  approximator_neurons, and autoregression.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"# Generate some data to use\nX₀, Y₀, X₁, Y₁ =  rand(1000, 5), rand(1000), rand(100, 5), rand(100)\n\n# We could also use DataFrames\n# using DataFrames\n# X₀ = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# X₁ = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# Y₀, Y₁ = DataFrame(y=rand(1000)), DataFrame(y=rand(1000))\n\nits = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)","category":"page"},{"location":"guide/its/#Step-2:-Estimate-the-Treatment-Effect","page":"Interrupted Time Series Estimation","title":"Step 2: Estimate the Treatment Effect","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Estimating the treatment effect only requires one argument: an InterruptedTimeSeries struct.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"estimate_causal_effect!(its)","category":"page"},{"location":"guide/its/#Step-3:-Get-a-Summary","page":"Interrupted Time Series Estimation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"We can get a summary of the model, including a p-value and statndard via asymptotic  randomization inference, by pasing the model to the summarize method.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Calling the summarize method returns a dictionary with the estimator's task (always  regression for interrupted time series analysis), whether the model uses an L2 penalty,  the activation function used in the model's outcome predictors, the validation metric used  for cross validation to find the best number of neurons, the number of neurons used in the  ELMs used by the estimator, the number of neurons used in the ELM used to learn a mapping  from number of neurons to validation loss during cross validation, the causal effect,  standard error, and p-value.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"summarize(its)","category":"page"},{"location":"guide/its/#Step-4:-Validate-the-Model","page":"Interrupted Time Series Estimation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"For an interrupted time series design to work well we need to be able to get an unbiased  prediction of the counterfactual outcomes. If the event or intervention effected the  covariates we are using to predict the counterfactual outcomes, then we will not be able to  get unbiased predictions. We can verify this by conducting a Chow Test on the covariates. An ITS design also assumes that any observed effect is due to the hypothesized intervention,  rather than any simultaneous interventions, anticipation of the intervention, or any  intervention that ocurred after the hypothesized intervention. We can use a Wald supremum  test to see if the hypothesized intervention ocurred where there is the largest structural  break in the outcome or if there was a larger, statistically significant break in the  outcome that could confound an ITS analysis. The covariates in an ITS analysis should be  good predictors of the outcome. If this is the case, then adding irrelevant predictors  should not have much of a change on the results of the analysis. We can conduct all these  tests in one line of code.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"tip: Tip\nOne can also specify the number of simulated confounders to generate to test the sensitivity  of the model to confounding and the minimum and maximum proportion of data to use in the  Wald supremum test by including the n, low, and high keyword arguments.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for interrupted time  series estimation. If the assumptions are not met then any estimates may be biased and  lead to incorrect conclusions.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nFor a review of interrupted time series identifying assumptions and robustness checks, see:Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single \ninterrupted time series design. No. w26080. National Bureau of Economic Research, 2019.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"validate(its)","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"<div style=\"width:100%; height:15px;;\n        border-radius:6px;text-align:center;\n        color:#1e1e20\">\n    <a class=\"github-button\" href=\"https://github.com/dscolby/CausalELM.jl\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star dscolby/CausalELM.jl on GitHub\" style=\"margin:auto\">Star</a>\n    <script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n</div>","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"#Overview","page":"CausalELM","title":"Overview","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM leverages new techniques in machine learning and statistics to estimate individual  and aggregate treatment effects in situations where traditional methods are unsatisfactory  or infeasible. To enable this, CausalELM provides a simple API to initialize a model,  estimate a causal effect, get a summary from the model, and test the robustness of the  model. CausalELM includes estimators for interupted time series analysis, G-Computation,  double machine learning, S-Learning, T-Learning, X-Learning, R-learning, and doubly robust  estimation. Underlying all these estimators are extreme learning machines. Like tree-based  learners, which are often used in causal machine learning, extreme learning machines are  simple and can capture non-linear relationships. However, unlike random forests or other  ensemble models, they essentially only require two hyperparameters—the number of neurons,  and the L2 penalty (when using regularization)—which are automatically tuned when  estimatecausaleffect! is called. This makes CausalELM both very simple and very powerful  for estimating treatment effects.","category":"page"},{"location":"#Features","page":"CausalELM","title":"Features","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Estimate a causal effect, get a summary, and validate assumptions in just four lines of code\nAll models automatically select the best number of neurons and L2 penalty\nEnables using the same structs for regression and classification\nIncludes 13 activation functions and allows user-defined activation functions\nMost inference and validation tests do not assume functional or distributional forms\nImplements the latest techniques form statistics, econometrics, and biostatistics\nWorks out of the box with DataFrames or arrays\nCodebase is high-quality, well tested, and regularly updated","category":"page"},{"location":"#What's-New?","page":"CausalELM","title":"What's New?","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Now includes doubly robust estimator for CATE estimation\nUses generalized cross validation with successive halving to find the best ridge penalty\nDouble machine learning, R-learning, and doubly robust estimators suppot specifying confounders and covariates of interest separately\nCounterfactual consistency validation simulates outcomes that violate the assumption rather than the previous binning approach\nStandardized and improved docstrings and added doctests\nCausalELM talk has been accepted to JuliaCon 2024!","category":"page"},{"location":"#What-makes-CausalELM-different?","page":"CausalELM","title":"What makes CausalELM different?","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Other packages, mainly EconML, DoWhy, CausalAI, and CausalML, have similar funcitonality.  Beides being written in Julia rather than Python, the main differences between CausalELM and  these libraries are:","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Simplicity is core to casualELM's design philosophy. CausalELM only uses one type of   machine learning model, extreme learning machines (with optional L2 regularization) and    does not require you to import any other packages or initialize machine learning models,    pass machine learning structs to CausalELM's estimators, convert dataframes or arrays to    a special type, or one hot encode categorical treatments. By trading a little bit of    flexibility for a simpler API, all of CausalELM's functionality can be used with just    four lines of code.\nAs part of this design principle, CausalELM's estimators handle all of the work in    finding the best number of neurons during estimation. They create folds or rolling    rolling for time series data and use an extreme learning machine interpolator to find    the best number of neurons.\nCausalELM's validate method, which is specific to each estimator, allows you to validate    or test the sentitivity of an estimator to possible violations of identifying assumptions.\nUnlike packages that do not allow you to estimate p-values and standard errors, use    bootstrapping to estimate them, or use incorrect hypothesis tests, all of CausalELM's    estimators provide p-values and standard errors generated via approximate randomization    inference. \nCausalELM strives to be lightweight while still being powerful and therefore does not    have external dependencies: all the functions it uses are in the Julia standard library.\nThe other packages and many others mostly use techniques from one field. Instead,    CausalELM incorporates a hodgepodge of ideas from statistics, machine learning,    econometrics, and biostatistics.","category":"page"},{"location":"#Installation","page":"CausalELM","title":"Installation","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM requires Julia version 1.7 or greater and can be installed from the REPL as shown  below. ","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"using Pkg \nPkg.add(\"CausalELM\")","category":"page"},{"location":"guide/metalearners/#Metalearners","page":"Metalearners","title":"Metalearners","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Instead of knowing the average causal effect, we might want to know which units benefit and  which units lose by being exposed to a treatment. For example, a cash transfer program might  motivate some people to work harder and incentivize others to work less. Thus, we might want  to know how the cash transfer program affects individuals instead of it average affect on  the population. To do so, we can use metalearners. Depending on the scenario, we may want to  use an S-learner, T-learner, X-learner, R-learner, or doubly robust learner. The basic steps  to use all five metalearners are below. The difference between the metalearners is how they  estimate the CATE and what types of variables they can handle. In the case of S, T, X, and  doubly robust learners, they can only handle binary treatments. On the other hand,  R-learners can handle binary, categorical, count, or continuous treatments but only supports  continuous outcomes.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nIf regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nFor a deeper dive on S-learning, T-learning, and X-learning see:Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.To learn more about R-learning see:Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment \neffects.\" Biometrika 108, no. 2 (2021): 299-319.To see the details out doubly robust estimation implemented in CausalELM see:     Kennedy, Edward H. \"Towards optimal doubly robust estimation of heterogeneous causal      effects.\" Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.","category":"page"},{"location":"guide/metalearners/#Initialize-a-Metalearner","page":"Metalearners","title":"Initialize a Metalearner","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"S-learners, T-learners, X-learners, R-learners, and doubly robust estimators all take at  least three arguments: an array of covariates, a vector of outcomes, and a vector of  treatment statuses. S, T, X, and doubly robust learners support binary treatment variables  and binary, continuous, count, or time to event outcomes. The R-learning estimator supports  binary, continuous, or count treatment variables and binary, continuous, count, or time to  event outcomes.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nInternally, the outcome and treatment models of the metalearners are treated as a regression  since extreme learning machines minimize the MSE. This means that predicted treatments and  outcomes under treatment and control groups could fall outside [0, 1], although this is not  likely in practice. To deal with this, predicted binary variables are automatically clipped to  [0.0000001, 0.9999999].This also means that count outcomes will be predicted as continuous  variables.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"tip: Tip\nAdditional options can be specified for each type of metalearner using its keyword arguments.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"# Generate data to use\nX, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]\n\n# We can also speficy potential confounders that we are not interested in\nW = randn(1000, 6)\n\n# We could also use DataFrames\n# using DataFrames\n# X = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:1000]), DataFrame(y=rand(1000))\n\ns_learner = SLearner(X, Y, T)\nt_learner = TLearner(X, Y, T)\nx_learner = XLearner(X, Y, T)\nr_learner = RLearner(X, Y, T, W=W)\ndr_learner = DoublyRobustLearner(X, T, Y, W=W)","category":"page"},{"location":"guide/metalearners/#Estimate-the-CATE","page":"Metalearners","title":"Estimate the CATE","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can estimate the CATE for all the models by passing them to estimatecausaleffect!.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"estimate_causal_effect!(s_learner)\nestimate_causal_effect!(t_learner)\nestimate_causal_effect!(x_learner)\nestimate_causal_effect!(r_learner)\nestimate_causal_effect!(dr_lwarner)","category":"page"},{"location":"guide/metalearners/#Get-a-Summary","page":"Metalearners","title":"Get a Summary","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can get a summary of the models that includes p0values and standard errors for the  average treatment effect by passing the models to the summarize method.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Calling the summarize methodd returns a dictionary with the estimator's task (regression or  classification), the quantity of interest being estimated (CATE), whether the model  uses an L2 penalty, the activation function used in the model's outcome predictors, whether  the data is temporal, the validation metric used for cross validation to find the best  number of neurons, the number of neurons used in the ELMs used by the estimator, the number  of neurons used in the ELM used to learn a mapping from number of neurons to validation  loss during cross validation, the causal effect, standard error, and p-value for the ATE.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"summarize(s_learner)\nsummarize(t_learner)\nsummarize(x_learner)\nsummarize(r_learner)\nsummarize(dr_learner)","category":"page"},{"location":"guide/metalearners/#Step-4:-Validate-the-Model","page":"Metalearners","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we simulate counterfactual outcomes that are  different from the observed outcomes, estimate models with the simulated counterfactual  outcomes, and take the averages. If the outcome is continuous, the noise for the simulated  counterfactuals is drawn from N(0, dev) for each element in devs, otherwise the default is  0.25, 0.5, 0.75, and 1.0 standard deviations from the mean outcome. For discrete variables,  each outcome is replaced with a different value in the range of outcomes with probability ϵ  for each ϵ in devs, otherwise the default is 0.025, 0.05, 0.075, 0.1. If the average  estimate for a given level of violation differs greatly from the effect estimated on the  actual data, then the model is very sensitive to violations of the counterfactual  consistency assumption for that level of violation. Next, this method tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"tip: Tip\nOne can also specify the maxium number of possible treatments to consider for the causal  consistency assumption and the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for interrupted time  series estimation. If the assumptions are not met then any estimates may be biased and  lead to incorrect conclusions.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"validate(s_learner)\nvalidate(t_learner)\nvalidate(x_learner)\nvalidate(r_learner)\nvalidate(dr_learner)","category":"page"}]
}
